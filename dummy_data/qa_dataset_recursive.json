{
    "queries": {
        "9336b433-75b7-4b7b-a7ca-f838009455a6": "Explain the concept of multilingual denoising pre-training and its significance in machine translation tasks.",
        "d72faa12-5169-4152-b236-6ca297bc59cd": "How does mBART differ from previous approaches in pre-training sequence-to-sequence models for machine translation?",
        "a721cbcf-84b2-4243-b07e-2a50c12ca70e": "Discuss the benefits of pre-training a complete autoregressive model like mBART for supervised and unsupervised machine translation tasks.",
        "a4734c60-f03a-4b67-a771-eb1cd84a6f4e": "How does mBART initialization impact performance in low-resource machine translation settings compared to high-resource settings?",
        "98a1132e-aee9-4b2d-a805-03583d13c641": "Describe the training process of mBART and how it is applied to large-scale monolingual corpora across multiple languages.",
        "0a360840-a02a-4212-a680-998d3683cd0e": "How does mBART initialization impact supervised sentence-level machine translation, particularly in low/medium-resource pairs?",
        "24420e01-41d0-4be9-84b1-89e45d9d3132": "What are the benefits of incorporating back-translation (BT) in conjunction with mBART initialization for machine translation tasks?",
        "4db872bc-c7f3-41b9-aafd-2fa50e373333": "How does document-level pre-training improve results in document-level machine translation?",
        "38f459b3-e5dd-49fd-81a0-7087d3d72d64": "What are the key findings regarding the performance of mBART in unsupervised machine translation, especially for less related language pairs?",
        "cbd986c8-2773-44dd-9648-dc0fa12d025f": "How does mBART compare to previous pre-training schemes in terms of performance across various machine translation tasks?",
        "041e36e5-4c77-4a5b-b072-8b43fbc5514a": "How does fine-tuning on bi-text in one language pair, such as Korean-English, enable a model to translate from all other languages in the monolingual pre-training set, like Italian-English, without further training?",
        "fb98d22a-5ada-4fa6-9df4-66d752a0f5bd": "What is the significance of the CC25 corpus in the multilingual denoising pre-training process for BART models?",
        "c6a0e42d-ad35-40f0-b562-db6ccb8944fc": "How does the tokenization with a sentence-piece model support fine-tuning on additional languages in the pre-training process?",
        "cd53688c-cd00-4dae-bd18-3e6df2e4ad61": "Explain the architecture of the mBART model and its components, such as the sequence-to-sequence Transformer architecture and the number of layers in the encoder and decoder.",
        "2265ccd8-924e-448f-9c6e-2bf6ddc38179": "What is the architecture used for the sequence-to-sequence Transformer in this study?",
        "7a5e18ef-0aef-402a-aa93-ac29186ed15c": "How many layers of encoder and decoder are included in the Transformer architecture?",
        "77124637-58fa-4562-8541-7a77fe09206b": "What is the model dimension used in the Transformer architecture?",
        "7dabec49-ac3a-4e3e-b023-38e66ec1e436": "How many parameters are there in the Transformer architecture used in this study?",
        "c6ecca4c-19d1-41cb-b816-066702dece2a": "What additional layer is included on top of both the encoder and decoder in the Transformer architecture?",
        "7fae64ba-b940-420f-9fb7-9a0d2242d1c7": "How many languages are covered in the training data D?",
        "ae9feef4-2a08-4dee-9fea-419136e40d56": "What is the purpose of the noising function g in the training process?",
        "1e3d142e-32e2-46c4-a857-87cd03898e4f": "How is the model trained to predict the original text X given g(X)?",
        "dc5005ae-0a7e-4497-9341-2e6d3e9aeb39": "What is the formula for maximizing L\u03b8 in the training process?",
        "c30b9fe6-750e-4fdc-baf4-54fd59eb271e": "How is the distribution P defined in the Seq2Seq model for training?",
        "6d4567ed-0249-4352-995b-1beb8ce0cd6b": "Explain the noise function used in the Transformer EncoderTransformer Decoder model for Multilingual Denoising Pre-Training. How does it contribute to the training process?",
        "0f945de9-813e-4106-8d31-89d88319c952": "Describe the instance format used for each batch in the pre-training process. How does it enable the model to work on both sentence and document translation?",
        "9cbace1b-48bd-4142-ac42-b003f4ba2795": "What optimization techniques were employed during the training of the full model, including the number of GPUs used, batch size, optimizer, and training time?",
        "eea4896a-1ef6-4aba-962b-22e12721df87": "Compare and contrast the mBART25 and mBART06 pre-trained models in terms of the languages they were trained on and the effects of multilinguality during pre-training.",
        "7f383261-7e13-4e5f-a2fc-75291ca67758": "Discuss the different types of noise used in the model, as well as the potential for exploring other noise types in future work. How do these noise types impact the training process?",
        "323d8927-81db-444b-9535-8f3637917675": "How does the training process for the models described in the text evolve in terms of dropout rate over time?",
        "f4b418ed-a6f0-4bc4-8835-234ad9c0755f": "What are the differences between the mBART25, mBART06, mBART02, and BART-En/Ro models in terms of the languages they are pre-trained on and the batch sizes used?",
        "b6b0e8f1-929e-439e-92d6-1aa78fe6029c": "How does the vocabulary used in all models contribute to improving generalization in multilingual settings, according to the text?",
        "8c6eea71-827f-4ae5-9272-5325d186a5a1": "What is the purpose of training monolingual BART models on the same English and Romanian corpus in the context of the study?",
        "9ff3d80c-991f-49ec-862e-14015af1a1c0": "How do the hyper-parameters, such as architecture and dropout, play a role in finding the best non-pretrained configuration for the models in the study?",
        "25e30c6b-6f33-4c91-b2bd-524425653842": "How does mBART pre-training compare to randomly initialized baselines in low to medium resource machine translation settings?",
        "7f6a7de2-02bd-48bc-96fe-e9c9b49fae78": "What types of datasets were used in the experimental settings for sentence-level machine translation evaluation?",
        "2945e023-781f-4342-9f2a-ed4777b8b384": "Can you explain the significance of back translation in the context of sentence-level machine translation?",
        "8cf19e57-1f22-4668-8528-b330bfda48d5": "How does pre-training contribute to performance gains in languages not present in the pre-training data at all?",
        "e0e624a4-4692-4e52-9171-bae70107a0a9": "Discuss the importance of parallel corpora in the context of machine translation research.",
        "7d45a0b7-d272-4780-94da-6a183cb752ff": "How are the datasets divided in terms of resource availability for the multilingual pre-trained models in the experimental settings?",
        "c12b6605-69c8-42a5-a358-0da53d5fd450": "Describe the process of fine-tuning and decoding the multilingual pre-trained models on a single pair of bilingual data.",
        "a450bcee-5269-4074-9f22-0f2ec9478932": "What are the key parameters used during training, such as dropout, label smoothing, warm-up steps, and maximum learning rate?",
        "3de49368-4742-4d64-9d06-e52e8086a930": "How are the final models selected and evaluated based on validation likelihood in the study?",
        "1b99e053-5a71-42a4-a3d8-1ff75f006bd1": "Discuss the observed gains in BLEU scores when initializing with the pre-trained mBART25 weights compared to randomly initialized baselines for low and medium resource pairs.",
        "b4e46503-4165-4675-877e-c29b0ef2627a": "Why does fine-tuning fail in extremely low-resource settings, as mentioned in the context information?",
        "59faca2c-5dab-43c5-a772-e1f4c553eeb2": "How does back-translation (BT) contribute to improving translation performance in low-resource language pairs, as demonstrated in the FLoRes dataset?",
        "3572f92e-fc8b-422b-a5d1-cb7d9cdc4fb8": "Compare the performance of mBART with other pre-training approaches on the WMT16 Ro-En translation task. What advantages does mBART demonstrate in this comparison?",
        "17524e9d-f6ab-4e87-93c3-642efadd8e38": "In what scenarios does pre-training on multiple languages prove to be beneficial, according to the analysis presented in the document? Provide examples to support your answer.",
        "3b720eb3-d2bc-40eb-87c7-ee86a75037df": "How does pre-training on multiple languages impact the performance of the model, particularly when the target language has limited monolingual data versus when it has plentiful monolingual data?",
        "45cad20b-78da-4d9c-bddc-cf6860a9bc9e": "In the context of pre-training, why does pre-training with similar languages seem to be particularly helpful, as indicated by the performance of mBART06 and mBART02 on the Ro-En pair?",
        "cfc12f43-58f2-453c-8a3a-64abc896fef8": "How does the number of pre-training steps affect the performance of the model, as shown in the plot of Ro-En BLEU score versus pre-training steps in Figure 3?",
        "dda48c91-d4ab-4954-9dc0-9039a6a2de73": "How does the size of bitexts influence the gain from pre-training according to the information provided in the document?",
        "8373eeea-cbe0-4dc8-a2a7-7a9f6553b691": "Can you explain the trend of pre-training consistently improving for low and medium resource language pairs as mentioned in the document?",
        "510d4cc2-30c5-4f3b-9c19-f8402365f13d": "Is pre-training complementary to back-translation (BT) based on the comparison presented in Table 6 of the document?",
        "52fc3d0e-06df-49b4-96d8-d0c7d97db9c5": "How does the performance of pre-trained models compare to baseline models when varying sizes of En-De datasets are used for training, as shown in Figure 4?",
        "26d3a801-427a-462e-af96-2bab0f76205d": "What are the estimated training costs for back-translation (BT) compared to the training costs of mBART models, as discussed in the document?",
        "810bb79b-a473-41ef-94cc-7c0700408b0d": "How does mBART pre-training compare to BT in terms of performance on English to Malay translation and Malay to English translation, based on the given data?",
        "761cf9e5-72c2-4e67-8d7e-b2ba0d745403": "What potential advantage does BT have over mBART in terms of utilizing monolingual data for pre-training?",
        "05df18af-fe07-4c9b-acb6-11e21309d93b": "How does combining the mBART02 model with BT lead to further gains in translation performance, according to the information provided?",
        "d4e7b25a-2b5d-47fb-aaa5-f69bd4138da4": "What are the estimated training costs for BT compared to mBART, and how do these costs differ in terms of efficiency?",
        "68ed8737-49d3-491c-b683-a31dda733fba": "How does the study demonstrate the generalization of mBART to languages not included in the pre-training corpora, and what does this suggest about the universality of the pre-training process?",
        "28b2e5c5-7e7c-49d8-9ff1-8148a536d25f": "How does pre-training on different language pairs, such as Nl-En, Ar-En, and De-Nl, impact the generalization to unseen languages in machine translation models like mBART25, mBART06, and mBART02?",
        "59dc9b0f-0fb0-433f-a464-32c580255545": "What are the implications of including or excluding certain languages, such as Arabic (Ar), German (De), or Dutch (Nl), during pre-training on the performance of machine translation models like mBART06 and EnRo Bilingual?",
        "a526d108-2ce6-46ac-b74c-6a12a0c9cf80": "How do the results of fine-tuning on English-Romanian language pairs compare to pre-training on other languages when translating distantly related unseen languages like Arabic, German, and Dutch?",
        "a604537c-da99-469c-96d4-a72877a0f21b": "What role do unseen vocabularies and universal properties of language play in the performance of machine translation models like mBART02 and mBART06 on language pairs with minimal lexical overlap?",
        "938abb27-a097-4e9d-9e2b-535227a1a1b6": "How does the performance of machine translation models differ when the unseen languages are on the source side, target side, or both sides, and what implications does this have for future research in fine-tuning unseen languages?",
        "beb095d3-6616-42a8-b89f-fc28058a5793": "How does pre-training with document fragments of up to 512 tokens improve document-level machine translation according to the study on mBART?",
        "ed9ee88e-75ed-44ea-9549-dc52a5ef5a17": "What datasets were used to evaluate the performance of mBART on document-level machine translation tasks?",
        "08ff7efd-9c0f-49aa-87e1-9e581d3b748a": "How is pre-processing done for document-level machine translation using mBART?",
        "2bde1bdd-b57d-4380-8abe-cb8d88803626": "What is the significance of splitting every document into 2-4 instances during pre-processing for document-level machine translation with mBART?",
        "bf2f8faf-19cc-4ba4-aab4-c1dd5171e197": "How does the fine-tuning scheme for document-level machine translation with mBART differ from that of sentence-level translation?",
        "34098566-e766-4bf8-b8e7-fa765fb558d8": "How do the sentence- and document-level BLEU scores differ for machine translation models on En-De and Zh-En languages?",
        "7292ff5c-aa59-47a1-9e1d-85ff65356563": "What are the main results presented in Table 9 for both En-De and Zh-En document-level machine translation?",
        "3a7df12c-435d-4ad2-b21a-9e6952b1194b": "Discuss the impact of pre-training on the performance of machine translation models, as highlighted in the context information.",
        "e5278668-376e-442e-814c-9a0e47e55fa0": "Compare the performance of mBART25 models with Hierarchical Attention Networks (HAN) on Zh-En document-level translation.",
        "dd732a98-ed63-4f5e-a98e-5ad86699ddfa": "Why is pre-training considered critical for document-level machine translation performance, according to the findings in the text?",
        "6c731b77-51d8-4679-b4a0-33c7988dfdfc": "Explain the importance of pre-training for document-level performance in machine translation based on the given information.",
        "67048347-be1b-4733-bde4-2074b0bbb123": "How does mBART provide a simple and effective initialization scheme for unsupervised machine translation methods using back-translation?",
        "42bbd6fa-e26a-4f15-9dd3-e553d18a6dc8": "Discuss the concept of zero-shot transfer in unsupervised machine translation and how it can be achieved through massively multi-lingual MT or distillation through pivoting.",
        "5fe721af-06bb-49d2-8212-35dad4e730d0": "What are the challenges faced in building machine translation models for single language pairs when no bi-text is available for the target pair?",
        "4c6d17b4-c498-410e-b01d-d61d8fb50416": "How is 3d-BLEU recomputed from the provided system output and why is it important in evaluating machine translation performance?",
        "72272d2d-8f90-4e3b-b461-703cf24cdfcc": "Explain the concept of unsupervised machine translation and discuss the two frameworks illustrated in Figure 5: back-translation and language transfer. How do these frameworks utilize multilingual pre-training, specifically mBART25?",
        "cc3bbe5c-f167-4e5b-af59-4419bee0eab2": "Describe the process of unsupervised machine translation via back-translation. How do pre-trained models perform on similar and dissimilar language pairs, and what are the results compared to non-pretrained models and existing pre-training methods?",
        "bed4b6a3-e0a9-4577-a26b-a86386da057a": "Discuss the second case of unsupervised machine translation via language transfer. How does this approach differ from back-translation, and what datasets are considered for X \u2192 En translation? What are the results of fine-tuning the pre-trained mBART25 model on different language pairs?",
        "d99be5c7-af29-407d-8166-64365cabeb03": "Compare the performance of models with multilingual pre-training to randomly initialized models without pre-training in the context of unsupervised machine translation. What does this comparison reveal about the importance of multilingual pre-training for achieving successful language transfer?",
        "45d294cf-2a04-474d-95a1-794ef4ee70a8": "How does the performance of the XLM model compare to other models in terms of similarity and dissimilarity pairs in various language pairs?",
        "ce5f0912-5f65-4837-aa4c-c6d62d293430": "What is the fine-tuning process for the mBART model in unsupervised machine translation, and how does it perform in different testing languages?",
        "2bb7e1f6-fba0-43c3-9930-f4e3bdda0d3d": "Explain the concept of unsupervised machine translation via language transfer, and provide an example of language transfer within similar language groups from the table.",
        "3ee56c02-ec7f-41b7-a040-5a478babc475": "Discuss the significance of back-translation in unsupervised machine translation and how it is utilized in the models mentioned in the context.",
        "4700b2b3-04bf-47e5-9ebd-2cbc9deda6db": "Compare the performance of different models in unsupervised machine translation based on their training on monolingual data and fine-tuning on specific language pairs.",
        "e9b8bdef-8b3e-488e-943e-7a4be0d05f8f": "How does language transfer play a role in unsupervised machine translation, and what are the key findings from the experiments conducted?",
        "2b8bfbe5-27a7-4fc4-91eb-2d80e6e56657": "In what scenarios is language transfer considered useful in the context of fine-tuning and back-translation for unsupervised MT?",
        "7ffc00a2-cac5-47b0-83d7-d53298b9ad7d": "Can you explain the significance of vocabulary sharing in language transfer, using examples from the document?",
        "62e07244-f653-4eb3-94fd-49d9f97c9483": "How do the results of combining back-translation and language transfer techniques compare to using them individually in unsupervised MT?",
        "94a777d0-e320-49c8-805a-b388051770fc": "Discuss the implications of the study's findings on the effectiveness of language transfer in machine translation, particularly in relation to similar language groups and vocabulary overlap.",
        "d9bf0574-233f-462a-84e3-6b1aa14e3d0b": "How does denoising pre-training contribute to improving translation applications, and how does it differ from traditional approaches?",
        "bafaac0c-72d3-406d-925e-595b24ca0595": "Discuss the significance of multilingual language learning in the field of NLP and its impact on translation models.",
        "dc63e7df-e4cb-4741-9ba6-aa3d5ae400db": "Explain the concept of multilingual translation and its benefits in improving translation performance for low-resource languages.",
        "8cb0a458-57b5-4725-b01e-46cd4ff92b60": "How does incorporating document-level contexts into neural machine translation enhance the translation process, and what are the challenges associated with it?",
        "3aec0c62-cc31-42de-a35e-b58277c218a7": "What are the different categories of unsupervised machine translation, and how do they address the issue of translating between languages without a direct parallel corpus?",
        "895dc213-4fa1-4bd4-b2fd-f84ce48c594f": "How do Artetxe et al. (2017) and Lample et al. (2018a,c) propose to improve machine translation when no parallel corpus is available? What limitations did they encounter with their approach?",
        "53f97bc5-07a1-4915-9c6d-3a3351032884": "How does Wu et al. (2019a) suggest replacing back-translation in the machine translation process? What is the alternative method they propose?",
        "cad3c50b-37d9-4cc0-b135-cdd4910941ea": "What is the approach taken by Pourdamghani et al. (2019) in their research on unsupervised translation using language transfer? How does it differ from other methods mentioned in the text?",
        "2ded7486-62f2-414b-a19c-9b71b49aafef": "How do the authors of the document demonstrate the effectiveness of multilingual de-noising pre-training in improving machine translation? What levels of translation do they analyze in their study?",
        "40bab1e1-05b2-44fe-a3c5-b3d365d02fcb": "How do the results of the study show the transfer learning ability of the learned representations from multilingual pre-training? What implications does this have for future research in machine translation?",
        "16ca4211-fab3-4737-878e-85104427de58": "What are some challenges associated with deploying the mBART100 model in production, and what is the proposed future work to address these challenges?",
        "b9888aac-a250-42ff-b32e-526a6930596d": "Who are some of the experts mentioned in the acknowledgements section, and what expertise did they provide in the field of low-resource and unsupervised machine translation?",
        "d73561c9-549a-4f19-9bed-9946844c6f59": "What are some key references cited in the document related to massively multilingual neural machine translation and unsupervised neural machine translation?",
        "7c30cc68-ce7b-44ba-b7cd-f57f8a397ac8": "What datasets were mentioned in the acknowledgements section, and who provided details about them?",
        "77713ba4-2bcf-444e-a556-2111fdeca698": "What are some key findings and challenges discussed in the document related to massively multilingual neural machine translation in the wild?",
        "4eb32494-c2ec-4f1a-9c66-08d240b228d4": "What is the significance of the WIT3 dataset mentioned in the references section, and how is it related to machine translation?",
        "bbe2d7cc-4d98-4e63-bfab-3a689542c19b": "What is the focus of the Facebook AI's WAT19 Myanmar-English translation task submission, and who were the authors involved in this work?",
        "0a4bafc4-8dd9-48e1-915c-38b594f34ddf": "What is the teacher-student framework for zero-resource neural machine translation, and what is its purpose in the field of machine translation?",
        "4d7ba5a4-a074-444f-b011-53104e67b871": "What is the main contribution of the paper titled \"Unsupervised cross-lingual representation learning at scale\" by Alexis Conneau et al.?",
        "f53954ee-63e2-4079-b686-8ef2f3999426": "What is the XNLI dataset mentioned in the references section, and how is it used to evaluate cross-lingual sentence representations?",
        "1c4e59b0-edba-4af5-ac18-109b5ede5eb3": "What is the significance of unsupervised cross-lingual representation learning at scale, as discussed in the paper by Conneau et al. (2019)?",
        "75048a0c-ed49-4d09-b5de-7b3ecae60c4f": "How does the Xnli dataset contribute to evaluating cross-lingual sentence representations, according to Conneau et al. (2018)?",
        "203822ba-29ac-4d44-93b0-82534bf47aeb": "Explain the concept of pre-training deep bidirectional transformers for language understanding, as outlined in the paper by Devlin et al. (2019).",
        "e91532da-a32e-491d-84c2-64c4319cb5b8": "What methods were proposed for Burmese (Myanmar) morphological analysis in the study by Ding et al. (2019)?",
        "b7ee426a-f591-4b26-8136-c79d04ef0e80": "Discuss the findings and contributions of the research on Burmese (Myanmar) morphological analysis in the ACM Transactions on Asian and Low-Resource Language Information Processing journal.",
        "c0c2bf5e-6922-4dc8-bd93-fa92ce75affe": "Question: What are some key advancements in natural language processing and machine translation discussed in the provided context information?",
        "4a55e876-ba63-4087-b39d-7f9d4212e533": "What are the key contributions of the IIT Bombay English-Hindi parallel corpus in the field of natural language processing?",
        "0599110d-91d6-4134-a3eb-54bffffa7142": "How does the approach of unsupervised machine translation using monolingual corpora only differ from traditional machine translation methods?",
        "20b06a08-b257-4448-9d86-49f16952ee1c": "What is the significance of word translation without parallel data in the context of cross-lingual language model pretraining?",
        "05e03b51-b193-4ba1-9040-d0707622969b": "How does the BART model improve natural language generation, translation, and comprehension compared to previous models?",
        "b4f34d6e-50c8-4fd9-a90f-9684d7ae135b": "Discuss the main findings and implications of the research on phrase-based & neural unsupervised machine translation.",
        "3dd7283a-b511-4dac-bfb3-3288bdedfcd7": "What are some key advancements in document-level neural machine translation as discussed in the provided context information?",
        "151f5ad9-3b30-49a8-90c0-53a743f0be6b": "How do pretrained language models play a role in text summarization, according to the information provided?",
        "684aa1b5-3243-47d0-a553-6c89aa925a04": "Describe the approach taken in the Roberta model for pretraining BERT, as outlined in the context.",
        "9348aa78-8dba-4399-8cda-209e89440b89": "How do hierarchical attention networks contribute to document-level neural machine translation, as mentioned in the text?",
        "85add476-0105-48f7-8737-c593a3ee7161": "What is the significance of BLEU scores in evaluating machine translation performance, based on the details provided?",
        "fbd84158-455d-49ca-9cf3-a77ca3db20bc": "Explain the concept of deep contextualized word representations and its relevance in computational linguistics, as discussed in the context.",
        "e0ea868d-b22b-4294-bd58-6c4d9e774707": "How does the FAIRSEQ toolkit contribute to sequence modeling, according to the information provided?",
        "e73f7c57-67d0-4e03-a9ca-e5257c47bc25": "Discuss the two-step approach to unsupervised machine translation proposed in the context.",
        "8e2318b1-9bc0-47aa-a5f5-aecd15dc971c": "What are some key findings from the study exploring the limits of transfer learning with a unified text-to-text transformer, as outlined in the context?",
        "46fb61b2-fcb9-4c57-91f1-7b852d6fef75": "How do language models serve as unsupervised multitask learners, as discussed in the provided information?",
        "f671b654-f92c-4956-ae5e-e0d700e4c2f6": "How do the Edinburgh neural machine translation systems for WMT 16 differ from traditional machine translation systems?",
        "a7204bb2-9d2f-483f-b69f-37b9930b5ef7": "What is the main contribution of the paper \"Ctrl: A conditional transformer language model for controllable generation\" by Nitish Shirish Keskar et al.?",
        "c3eec421-8eeb-4d7c-9001-0897dee9a747": "How does the MASS model, as described in the paper by Kaitao Song et al., improve language generation compared to other pre-training methods?",
        "403256a2-f4b1-4788-a494-ef320606505e": "What is the significance of the paper \"Neural machine translation with extended context\" by J\u00f6rg Tiedemann and Yves Scherrer in the field of machine translation?",
        "8fd7df84-0349-419a-9eaf-0ee9e14d7c20": "How does the unified text-to-text transformer approach in the paper by Rico Sennrich et al. push the limits of transfer learning in natural language processing?",
        "f6e5df59-f553-4f8d-a1da-54175398c406": "How is the translation performance evaluated in the document, and what metric is used for evaluation?",
        "8a8adc7d-c22c-4bc4-9576-db0f1b793718": "Can you explain the tokenization process for different languages mentioned in the document?",
        "95a0e4b3-9a2d-4003-b02b-8764daf0c38f": "What are some key techniques or models mentioned in the document for improving machine translation?",
        "082ff025-2049-4134-aafc-265c17c46123": "How do the authors handle cross-sentence context in neural machine translation?",
        "13344cab-1451-4c6e-9e71-643119b64d0c": "What is the significance of the continuous cache in learning translation history, as discussed in the document?",
        "3ec6abcb-89d6-48b4-ad8c-09a7a42e9c2c": "How is tokenization handled for Indic languages in the context of language-wise tokenization instructions?",
        "6f46469e-d6e5-4cb3-a712-06e39c68547b": "What tool is used to segment Japanese texts in the context of language-wise tokenization instructions?",
        "2b2e513b-e8ee-487a-9d6a-ea4e8520f06c": "How are Korean texts segmented in the context of language-wise tokenization instructions?",
        "f678dfc8-fb98-4245-b110-8ca384b462d3": "What normalization process is applied to Arabic texts in the context of language-wise tokenization instructions?",
        "3e980cee-b124-44ea-aea0-89739681d5b9": "How are Burmese texts segmented according to the context information provided?",
        "96e19324-83f1-4dcd-b884-52eb516ed016": "What tokenization and normalization methods are used for Romanian texts based on the context information?",
        "35a91271-1ab5-48dd-b12c-08a30c44b5c0": "Which Chinese tokenizer is utilized for tokenization in the context of language-wise tokenization instructions?",
        "a5b3f360-defe-48f7-b07a-abba001d54f6": "How are BLEU scores computed for languages not listed in the context information provided?",
        "e545fca2-101c-4625-af73-7b79b649ea68": "What is the purpose of the Indic-NLP Library mentioned in the context information?",
        "3450ff10-7b37-4d61-ba28-acdd3be1f9b7": "How does the sacreBLEU Chinese tokenizer differ from other tokenization methods mentioned in the context information?",
        "251a2097-b0e8-447a-934a-2b66c426492f": "How does the artist in the passage view the connection between humans and nature in their work?",
        "7cf11b42-0690-4cd4-8802-68b53a46c60d": "How does the artist in the passage view the connection between humans and nature, particularly in relation to icebergs?",
        "7eaa4245-3f97-4be8-80e4-0e1021f98982": "How does the artist in the passage emphasize the connection between humans and nature through their artwork?",
        "5c4d7010-ea72-495a-88de-0acc429d35dc": "How does the speaker describe the process of icebergs melting and the significance of this event in relation to life and continuation?",
        "41e0a88e-15c0-47ef-b6be-4a1e5760644c": "How does the speaker in the text describe the relationship between icebergs and the environment around them?",
        "e8bf3401-4fed-4e48-a3b2-6b66f75beb6f": "Question: How does the artist describe the connection between human beings and nature in their artwork?",
        "acfc22b1-9681-4e8c-a8bb-1f33f63aa0b5": "What is the significance of the icebergs mentioned in the context, particularly the one made in Greenland?",
        "d7154a11-8e6d-42b8-85ea-11d72b730003": "How does the shape of the iceberg change as it starts rolling in the water?",
        "e819bd68-a930-4aff-9c9f-cbe71d800ef2": "How tall is an average iceberg in Greenland, as mentioned in the context?",
        "31154a49-1309-47ba-b165-6322bb9e0ab3": "How does the artist in the context describe the personality of icebergs?",
        "c680bd09-fb26-4064-bdc4-9b4b64112d16": "What is the difference in translation quality between the Sent-MT and Doc-MT systems, as discussed in the context?",
        "e22cb956-dc41-4618-92c5-a4dbcf7dac5a": "What actions are being taken by various medical councils and associations in response to the government's silence regarding labor upgrades and workforce actions starting in September?",
        "ca4acfd6-3966-4390-960c-fefd1cbe4205": "What is the conventional wisdom regarding achieving higher resolution in the context of magnets?",
        "18d92181-732c-4a05-9c64-614f8cb1f22b": "How does the mBART25 model demonstrate unsupervised machine translation via language transfer between Japanese, Korean, and Chinese to English?",
        "923a4c74-adb0-4a67-897a-5fe51865888b": "Can you explain the failure cases observed in the examples provided, such as the incorrect translations of \"\uc790\uc11d\" and \"developed\" by the models?",
        "8e525723-0fe8-4566-8824-9536161736b4": "How does the assumption of cultural and historical correlation between languages play a role in the success of pre-training models like mBART25 in translation tasks?",
        "fc93ee87-fce0-4189-8d49-d82e52720b20": "Discuss the limitations of using bigger magnets or cushions to achieve higher resolution, as mentioned in the text."
    },
    "corpus": {
        "f918a968-3163-4413-87d4-e2fbee0e7e65": "Multilingual Denoising Pre-training for Neural Machine Translation\nYinhan Liu*, Jiatao Gu*, Naman Goyal*, Xian Li, Sergey Edunov\nMarjan Ghazvininejad, Mike Lewis, Luke Zettlemoyer\nFacebook AI Research\n{yinhanliu,jgu,naman,xianl,edunov\nghazvini,mikelewis,lsz} @fb.com\nAbstract\nThis paper demonstrates that multilingual\ndenoising pre-training produces signi\ufb01cant\nperformance gains across a wide variety of\nmachine translation (MT) tasks. We present\nmBART \u2013 a sequence-to-sequence denois-\ning auto-encoder pre-trained on large-scale\nmonolingual corpora in many languages us-\ning the BART objective (Lewis et al., 2019).\nmBART is the \ufb01rst method for pre-training\na complete sequence-to-sequence model by\ndenoising full texts in multiple languages,\nwhile previous approaches have focused\nonly on the encoder, decoder, or reconstruct-\ning parts of the text. Pre-training a complete\nmodel allows it to be directly \ufb01ne tuned\nfor supervised (both sentence-level and\ndocument-level) and unsupervised machine\ntranslation, with no task-speci\ufb01c modi\ufb01ca-\ntions. We demonstrate that adding mBART\ninitialization produces performance gains in\nall but the highest-resource settings, includ-\ning up to 12 BLEU points for low resource\nMT and over 5 BLEU points for many\ndocument-level and unsupervised models.\nWe also show it also enables new types of\ntransfer to language pairs with no bi-text or\nthat were not in the pre-training corpus, and\npresent extensive analysis of which factors\ncontribute the most to effective pre-training.\n1 Introduction\nDespite its wide adoption for other NLP tasks (De-\nvlin et al., 2019; Liu et al., 2019; Yang et al.,\n2019; Lewis et al., 2019; Raffel et al., 2019), self-\nsupervised pretraining is not yet common prac-\ntice in machine translation (MT). Existing MT\napproaches only pre-train parts of the model, in-\ncluding the encoder (Lample and Conneau, 2019)\nand the decoder (Edunov et al., 2019), or use pre-\ntraining objectives that only reconstruct parts of\ntext (Song et al., 2019), or only focus on English\n*Equal contribution.corpora (Lewis et al., 2019; Raffel et al., 2019). In\nthis paper, we show that signi\ufb01cant performance\ngains are possible by pre-training a complete au-\ntoregressive model with an objective that noises\nand reconstructs full texts across many languages.\nIn this work, we present mBART \u2013 a multilin-\ngual sequence-to-sequence (Seq2Seq) denoising\nauto-encoder. mBART is trained by applying the\nBART (Lewis et al., 2019) to large-scale mono-\nlingual corpora across many languages. The input\ntexts are noised by masking phrases and permut-\ning sentences, and a single Transformer (Vaswani\net al., 2017) model is learned to recover the texts.\nDifferent from other pre-training approaches for\nMT (Lample and Conneau, 2019; Song et al.,\n2019), mBART pre-trains a complete autoregres-\nsive Seq2Seq model. mBART is trained once for\nall languages, providing a set of parameters that\ncan be \ufb01ne-tuned for any of the language pairs in\nboth supervised and unsupervised settings, with-\nout any task-speci\ufb01c or language-speci\ufb01c modi\ufb01-\ncations or initialization schemes.\nExtensive experiments demonstrate that this\nsimple approach works remarkably well. We \ufb01rst\nfocus on existing MT benchmarks. For supervised\nsentence-level MT, mBART initialization leads to\nsigni\ufb01cant gains (up to 12 BLEU points) across\nlow/medium-resource pairs (<10M bi-text pairs),\nwithout sacri\ufb01cing performance in high-resource\nsettings. These results further improve with back-\ntranslation (BT), setting a new state-of-the-art on\nWMT16 English-Romanian and the FloRes test\nsets. For document-level MT, our document-level\npre-training improves results by up to 5.5.",
        "a20eca33-0755-40bf-bdd0-290713100348": "Extensive experiments demonstrate that this\nsimple approach works remarkably well. We \ufb01rst\nfocus on existing MT benchmarks. For supervised\nsentence-level MT, mBART initialization leads to\nsigni\ufb01cant gains (up to 12 BLEU points) across\nlow/medium-resource pairs (<10M bi-text pairs),\nwithout sacri\ufb01cing performance in high-resource\nsettings. These results further improve with back-\ntranslation (BT), setting a new state-of-the-art on\nWMT16 English-Romanian and the FloRes test\nsets. For document-level MT, our document-level\npre-training improves results by up to 5.5. For\nthe unsupervised case, we see consistent gains\nand produce the \ufb01rst non-degenerate results for\nless related language pairs (e.g., 9.5 BLEU gain\non Nepali-English). Previous pre-training schemes\nhave only considered subsets of these tasks, but we\ncompare performance where possible and demon-\nstrate that mBART consistently performs the best.arXiv:2001.08210v2  [cs.CL]  23 Jan 2020",
        "7ccd09b5-582d-4e4a-8da8-0eb6d5d1f823": "We also show that mBART enables new types\nof transfer across language pairs. For example,\n\ufb01ne-tuning on bi-text in one language pair (e.g.,\nKorean-English) creates a model that can trans-\nlate from all other languages in the monolingual\npre-training set (e.g., Italian-English), with no fur-\nther training. We also show that languages not\nin pre-training corpora can bene\ufb01t from mBART,\nstrongly suggesting that the initialization is at least\npartially language universal. Finally, we present a\ndetailed analysis of which factors contribute the\nmost to effective pre-training, including the num-\nber of languages and their overall similarity.\n2 Multilingual Denoising Pre-training\nWe use a large-scale common crawl (CC) corpus\n(\u00a72.1) to pre-train BART models (\u00a72.2). Our ex-\nperiments in the later sections involve \ufb01netuning a\nrange of models pre-trained on different subsets of\nthe CC languages \u00a72.3).\n2.1 Data: CC25 corpus\nDatasets We pre-train on a subset of 25 lan-\nguages \u2013 CC25 \u2013 extracted from the Common\nCrawl (CC) (Wenzek et al., 2019; Conneau et al.,\n2019)1. CC25 includes languages from different\nfamilies and with varied amounts of text (Table 1).\nFollowing Lample and Conneau (2019), we re-\nbalanced the corpus by up/down-sampling text\nfrom each language iwith a ratio\u03bbi:\n\u03bbi=1\npi\u00b7p\u03b1\ni\u2211\nip\u03b1\ni, (1)\nwherepiis the percentage of each language in CC-\n25. We use the smoothing parameter \u03b1= 0.7.\nPre-processing We tokenize with a sentence-\npiece model (SPM, Kudo and Richardson, 2018)\nlearned on the full CC data that includes 250,000\nsubword tokens. While not all of these languages\nare used for pre-training, this tokenization sup-\nports \ufb01ne-tuning on additional languages. We do\nnot apply additional preprocessing, such as true-\ncasing or normalizing punctuation/characters.\n2.2 Model: mBART\nOur models follow the BART (Lewis et al., 2019)\nsequence-to-sequence pre-training scheme, as re-\nviewed in this section. While BART was only pre-\ntrained for English, we systematically study the ef-\nfects of pre-training on different sets of languages.\n1https://commoncrawl.orgCode Language Tokens/M Size/GB\nEn English 55608 300.8\nRu Russian 23408 278.0\nVi Vietnamese 24757 137.3\nJa Japanese 530 (*) 69.3\nDe German 10297 66.6\nRo Romanian 10354 61.4\nFr French 9780 56.8\nFi Finnish 6730 54.3\nKo Korean 5644 54.2\nEs Spanish 9374 53.3\nZh Chinese (Sim) 259 (*) 46.9\nIt Italian 4983 30.2\nNl Dutch 5025 29.3\nAr Arabic 2869 28.0\nTr Turkish 2736 20.9\nHi Hindi 1715 20.2\nCs Czech 2498 16.3\nLt Lithuanian 1835 13.7\nLv Latvian 1198 8.8\nKk Kazakh 476 6.4\nEt Estonian 843 6.1\nNe Nepali 237 3.8\nSi Sinhala 243 3.6\nGu Gujarati 140 1.9\nMy Burmese 56 1.6\nTable 1: Languages and Statistics of the CC25 Cor-\npus. A list of 25 languages ranked with monolingual\ncorpus size. Throughout this paper, we replace the lan-\nguage names with their ISO codes for simplicity. (*)\nChinese and Japanese corpus are not segmented, so the\ntokens counts here are sentences counts\nArchitecture We use a standard sequence-to-\nsequence Transformer architecture (Vaswani et al.,\n2017), with 12layers of encoder and 12layers\nof decoder with model dimension of 1024 on16\nheads (\u223c680M parameters). We include an addi-\ntional layer-normalization layer on top of both the\nencoder and decoder, which we found stabilized\ntraining at FP16 precision.",
        "ab7f18d4-2e6b-4b9c-bfe6-cfc4dc2ef621": "A list of 25 languages ranked with monolingual\ncorpus size. Throughout this paper, we replace the lan-\nguage names with their ISO codes for simplicity. (*)\nChinese and Japanese corpus are not segmented, so the\ntokens counts here are sentences counts\nArchitecture We use a standard sequence-to-\nsequence Transformer architecture (Vaswani et al.,\n2017), with 12layers of encoder and 12layers\nof decoder with model dimension of 1024 on16\nheads (\u223c680M parameters). We include an addi-\ntional layer-normalization layer on top of both the\nencoder and decoder, which we found stabilized\ntraining at FP16 precision.\nLearning Our training data covers Klanguages:\nD={D1,...,DK}where eachDiis a collection\nof monolingual documents in language i. We (1)\nassume access to a noising function g, de\ufb01ned be-\nlow, that corrupts text, and (2) train the model to\npredict the original text Xgiveng(X). More for-\nmally, we aim to maximize L\u03b8:\nL\u03b8=\u2211\nDi\u2208D\u2211\nX\u2208D ilogP(X|g(X);\u03b8),(2)\nwhereXis an instance in language iand the dis-\ntributionPis de\ufb01ned by the Seq2Seq model.",
        "52b995ea-52d1-4771-a99b-5f22725c1720": "Where did __ from ? </s> Who __ I __ </s> <En> <En> Who am I ? </s> Where did I come from ? </s> Who am I ? </s> Where did I come from ? </s> <En> \nWho am I ? </s> <En> Transformer EncoderTransformer Decoder\u143a\u0003\u0385\u0003\u6291\u0003\u0498 </s> <Ja>\n<Ja> \u143a\u0003\u0385\u0003\u6291\u0003\u0498 </s> Transformer EncoderTransformer Decoder\n BB\u0003\u0e01\u0ded\u0003\u0336 </s> \u0373\u03a2\u0003BB\u001f\u0012V!\u0003<Ja> <Ja> \u0373\u03a2\u0003\u036e\u0399\u0003\u0358 \u0335\u001f\u0012V!\u0003\u0380\u0375\u0003\u0e01\u0ded\u0003\u0336 </s> \u0373\u03a2\u0003\u036e\u0399\u0003\u0358 \u0335\u001f\u0012V!\u0003\u0380\u0375\u0003\u0e01\u0ded\u0003\u0336 </s> <Ja> Transformer EncoderTransformer DecoderMultilingual Denoising Pre-Training  (mBART)Fine-tuning on Machine Translation\u0373\u03a2\u0003\u036e\u0399\u0003\u0358 \u0335\u001f\u0012V!\u0003\u0380\u0375\u0003\u0e01\u0ded\u0003\u0336 </s> <Ja> Transformer EncoderTransformer Decoder:HOO\u0003WKHQ\u0003\u0011\u0003\u001f\u0012V! See you tomorrow .</s> <En>\n<En> :HOO\u0003WKHQ\u0003\u0011\u0003\u001f\u0012V! See you tomorrow .</s> Doc-MTSent-MTFigure 1: Framework for our Multilingual Denoising Pre-training (left) and \ufb01ne-tuning on downstream MT tasks\n(right), where we use (1) sentence permutation (2) word-span masking as the injected noise. A special language id\ntoken is added at both the encoder and decoder. One multilingual pre-trained model is used for all tasks.\nNoise function Following Lewis et al. (2019),\nwe use two types of noise in g. We \ufb01rst remove\nspans of text and replace them with a mask to-\nken. We mask 35% of the words in each instance\nby random sampling a span length according to a\nPoisson distribution ( \u03bb= 3.5). We also permute\nthe order of sentences within each instance. The\ndecoder input is the original text with one posi-\ntion offset. A language id symbol <LID> is used\nas the initial token to predict the sentence. It is also\npossible to use other noise types, such as those in\nLample et al. (2018c), but we leave the exploration\nof the optimal noising strategy to future work.\nInstance format For each instance of a batch,\nwe sample a language id symbol <LID>, and\nwe pack as many consecutive sentences as pos-\nsible sampled from the corresponding corpus of\n<LID>, until either it hits the document boundary\nor reaches the 512 max token length. Sentences\nin the instance are separated by the end of sen-\ntence (</S>) token. Then, we append the selected\n<LID> token to represent the end of this instance.\nPre-training at \u201cmulti-sentence\u201d level enables us to\nwork on both sentence and document translation.\nOptimization Our full model (including 25lan-\nguages) is trained on 256 Nvidia V100 GPUs\n(32GB) for 500K steps. The total batch size\nis around 128K tokens per GPU, matching\nBART (Lewis et al., 2019) con\ufb01guration. We use\nthe Adam optimizer ( \u03f5= 1e\u22126,\u03b22= 0.98) and\nlinear learning rate decay scheduling. The total\ntraining time was approximately 2.5 weeks. We\nstarted the training with dropout 0.1and reduced it\nto0.05at 250K steps and 0at 400K steps. All ex-\nperiments are done with Fairseq (Ott et al., 2019).2.3 Pre-trained Models\nTo better measure the effects of different levels\nof multilinguality during pre-training, we built a\nrange of models as follows:\n\u2022mBART25 We pre-train a model on all 25 lan-\nguages, using the setting described in \u00a72.2.\n\u2022mBART06 To explore the effect of pre-training\non related languages, we pretrain a model on a\nsubset of six European languages: Ro, It, Cs, Fr,\nEs and En.",
        "9e3dcb5a-58cb-4bbe-af09-fce0e96bda9f": "The total\ntraining time was approximately 2.5 weeks. We\nstarted the training with dropout 0.1and reduced it\nto0.05at 250K steps and 0at 400K steps. All ex-\nperiments are done with Fairseq (Ott et al., 2019).2.3 Pre-trained Models\nTo better measure the effects of different levels\nof multilinguality during pre-training, we built a\nrange of models as follows:\n\u2022mBART25 We pre-train a model on all 25 lan-\nguages, using the setting described in \u00a72.2.\n\u2022mBART06 To explore the effect of pre-training\non related languages, we pretrain a model on a\nsubset of six European languages: Ro, It, Cs, Fr,\nEs and En. For a fair comparison, we use \u223c1/4\nof the mBART25 batch size, which allows our\nmodel to have the same number of updates per\nlanguage during pre-training.\n\u2022mBART02 We pre-train bilingual models, us-\ning English and one other language for four\nlanguage pairs: En-De, En-Ro, En-It. We use a\nbatch size of\u223c1/12of that in the mBART25.\n\u2022BART-En/Ro To help establish baseline per-\nformance levels, we also train monolingual\nBART models on the same En and Ro corpus\nonly.\n\u2022Random As additional baselines, we will also\ninclude a comparison with a model randomly\ninitialized without pre-training for each trans-\nlation task. Since the sizes of different down-\nstream datasets vary, we always grid-search the\nhyper-parameters (architecture, dropout, etc.) to\n\ufb01nd the best non-pretrained con\ufb01guration.\nAll models use the same vocabulary (\u00a72.1). Not\nall tokens will frequently occur in all pre-training\ncorpora, but later experiments show that this large\nvocabulary can improve generalization in multilin-\ngual settings even for unseen languages.",
        "5dde4e00-4f9c-4428-a279-33a1b97fbffd": "Languages En-Gu En-Kk En-Vi En-Tr En-Ja En-Ko\nData Source WMT19 WMT19 IWSLT15 WMT17 IWSLT17 IWSLT17\nSize 10K 91K 133K 207K 223K 230K\nDirection\u2190 \u2192 \u2190 \u2192 \u2190 \u2192 \u2190 \u2192 \u2190 \u2192 \u2190 \u2192\nRandom 0.0 0.0 0.8 0.2 23.6 24.8 12.2 9.5 10.4 12.3 15.3 16.3\nmBART25 0.3 0.1 7.4 2.5 36.1 35.4 22.5 17.8 19.1 19.4 24.6 22.6\nLanguages En-Nl En-Ar En-It En-My En-Ne En-Ro\nData Source IWSLT17 IWSLT17 IWSLT17 WAT19 FLoRes WMT16\nSize 237K 250K 250K 259K 564K 608K\nDirection\u2190 \u2192 \u2190 \u2192 \u2190 \u2192 \u2190 \u2192 \u2190 \u2192 \u2190 \u2192\nRandom 34.6 29.3 27.5 16.9 31.7 28.0 23.3 34.9 7.6 4.3 34.0 34.3\nmBART25 43.3 34.8 37.6 21.6 39.8 34.0 28.3 36.9 14.5 7.4 37.8 37.7\nLanguages En-Si En-Hi En-Et En-Lt En-Fi En-Lv\nData Source FLoRes ITTB WMT18 WMT19 WMT17 WMT17\nSize 647K 1.56M 1.94M 2.11M 2.66M 4.50M\nDirection\u2190 \u2192 \u2190 \u2192 \u2190 \u2192 \u2190 \u2192 \u2190 \u2192 \u2190 \u2192\nRandom 7.2 1.2 10.9 14.2 22.6 17.9 18.1 12.1 21.8 20.2 15.6 12.9\nmBART25 13.7 3.3 23.5 20.8 27.8 21.4 22.4 15.3 28.5 22.4 19.3 15.9\nTable 2: Low/Medium Resource Machine Translation Pre-training consistently improves over a randomly ini-\ntialized baseline, with particularly large gains on low resource language pairs (e.g. Vi-En).\nLanguages Cs Es Zh De Ru Fr\nSize 11M 15M 25M 28M 29M 41M\nRandom 16.5 33.2 35.0 30.9 31.5 41.4\nmBART25 18.0 34.0 33.3 30.5 31.3 41.0\nTable 3: High Resource Machine Translation where\nall the datasets are from their latest WMT competitions.\nWe only evaluate our models on En-X translation.\n3 Sentence-level Machine Translation\nThis section shows that mBART pre-training pro-\nvides consistent performance gains in low to\nmedium resource sentence-level MT settings, in-\ncluding bi-text only and with back translation, and\noutperforms other existing pre-training schemes\n(\u00a73.2). We also present a detailed analysis to un-\nderstand better which factors contribute the most\nto these gains (\u00a73.3), and show that pre-training\ncan even improve performance for languages not\npresent in the pre-training data at all (\u00a73.4).\n3.1 Experimental Settings\nDatasets We gather 24pairs of publicly avail-\nable parallel corpora that cover all the languages\nin CC25 (Table 1). Most pairs are from previous\nWMT (Gu, Kk, Tr, Ro, Et, Lt, Fi, Lv, Cs, Es,\nZh, De, Ru, Fr\u2194En) and IWSLT (Vi, Ja, Ko,\nNl, Ar, It\u2194En) competitions. We also use FLo-\nRes pairs (Guzm\u00e1n et al., 2019, En-Ne and En-\nSi), En-Hi from IITB (Kunchukuttan et al., 2017),and En-My from WAT19 (Ding et al., 2018, 2019).",
        "b3b5059f-f2fa-48c2-8422-67746f294be0": "3.1 Experimental Settings\nDatasets We gather 24pairs of publicly avail-\nable parallel corpora that cover all the languages\nin CC25 (Table 1). Most pairs are from previous\nWMT (Gu, Kk, Tr, Ro, Et, Lt, Fi, Lv, Cs, Es,\nZh, De, Ru, Fr\u2194En) and IWSLT (Vi, Ja, Ko,\nNl, Ar, It\u2194En) competitions. We also use FLo-\nRes pairs (Guzm\u00e1n et al., 2019, En-Ne and En-\nSi), En-Hi from IITB (Kunchukuttan et al., 2017),and En-My from WAT19 (Ding et al., 2018, 2019).\nWe divide the datasets into three categories \u2013 low\nresource (<1M sentence pairs), medium resource\n(>1M and<10M), and high resource ( >10M).\nFine-tuning & Decoding We \ufb01ne-tune our mul-\ntilingual pre-trained models on a single pair of bi-\ntext data, feeding the source language into the en-\ncoder and decoding the target language. As shown\nin Figure 1, we load the pre-trained weights and\ntrain the MT model on bi-texts with teacher forc-\ning. For all directions, we train with 0.3dropout,\n0.2label smoothing, 2500 warm-up steps, 3e\u22125\nmaximum learning rate. We use a maximum of\n40K training updates for all low and medium re-\nsource pairs and 100K for high resource pairs. The\n\ufb01nal models are selected based on validation like-\nlihood. For decoding, we use beam-search with\nbeam size 5for all directions. The \ufb01nal results\nare reported in BLEU (Papineni et al., 2002) with\nlanguage-speci\ufb01c settings, see appendix A.\n3.2 Main Results\nAs shown in Table 2, initializing with the pre-\ntrained mBART25 weights shows gains on all the\nlow and medium resource pairs when compared\nwith randomly initialized baselines. We observe\ngains of 12+ BLEU on low resource pairs such as\nEn-Vi, En-Tr, and noisily aligned pairs like En-Hi.\nFine-tuning fails in extremely low-resource setting\nsuch as En-Gu, which only have roughly 10k ex-",
        "7048f69f-cc2a-41b4-876d-c2a5e40491c0": "0 1 2\n+BT iterations6810Finetuning BLEU\n4.36.8 6.87.48.69.6En-Ne\nRandom\nmBART25\n0 1 2\n+BT iterations101520\n7.612.715.114.519.421.3Ne-En\nRandom\nmBART25\n0 1 2\n+BT iterations0246810\n1.25.26.5\n3.37.99.3En-Si\nRandom\nmBART25\n0 1 2\n+BT iterations101520\n7.212.115.1\n13.718.620.2Si-En\nRandom\nmBART25Figure 2: Pre-training + Back Translation on FLoRes with two iterations of BT.\nPre-training Fine-tuning\nModel Data En\u2192Ro Ro\u2192En +BT\nRandom None 34.3 34.0 36.8\nXLM (2019) En Ro - 35.6 38.5\nMASS (2019) En Ro - - 39.1\nBART (2019) En - - 38.0\nXLM-R (2019) CC100 35.6 35.8 -\nBART-En En 36.0 35.8 37.4\nBART-Ro Ro 37.6 36.8 38.1\nmBART02 En Ro 38.5 38.5 39.9\nmBART25 CC25 37.7 37.8 38.8\nTable 4: Comparison with Other Pre-training Ap-\nproaches on WMT16 Ro-En.\namples for tuning. In these settings, unsupervised\ntranslation is more appropriate, see \u00a75.2.\nFor high resource cases (Table 3), we do not\nobserve consistent gains, and pre-training slightly\nhurts performance when >25M parallel sentence\nare available. When a signi\ufb01cant amount of bi-text\ndata is given, we suspect that supervised training\nwashes out the pre-trained weights completely.\n+ Back Translation Back-translation (BT, Sen-\nnrich et al., 2016b) is a standard approach to aug-\nment bi-text with target side monolingual data. We\ncombine our pre-training with BT and test it on\nlow resource language pairs \u2013 En-Si and En-Ne \u2013\nusing the FLoRes dataset (Guzm\u00e1n et al., 2019).\nFor a fair comparison, we use the same mono-\nlingual data as (Guzm\u00e1n et al., 2019) to gener-\nate BT data. Figure 2 shows that initializing the\nmodel with our mBART25 pre-trained parameters\nimproves BLEU scores at each iteration of back\ntranslation, resulting in new state-of-the-art results\nin all four translation directions.\nv.s. Other Pre-training Approaches We also\ncompare our pre-trained models with recent self-\nsupervised pre-training methods, as shown in Ta-\nble 4. We consider En-Ro translation, the only\npair with established results. Our mBART modeloutperforms all the other pre-trained models, both\nwith and without BT augmentation. We also show\ncomparisons with the conventional BART model\ntrained on the same En and Ro data only. Both\nhave improvements over baselines, while worse\nthan mBART results, indicating pre-training in a\nmultilingual setting is essential. Moreover, com-\nbining BT leads to additional gains, resulting in a\nnew state-of-the-art for Ro-En translation.\n3.3 Analysis\nWe also present additional analysis, to better quan-\ntify when our pre-training helps.\nHow many languages should you pre-train on?\nWe investigate when it is helpful for pre-training\nto include languages other than the targeted lan-\nguage pair that will be used during \ufb01ne tuning. Ta-\nble 5 shows performance on four X-En pairs. Pre-\ntraining on more languages helps most when the\ntarget language monolingual data is limited (e.g.\nEn-My, the size of My is around 0.5%of En).\nIn contrast, when monolingual data is plenti-\nful (De, Ro), pre-training on multiple languages\nslightly hurts the \ufb01nal results (< 1BLEU). In these\ncases, additional languages may reduce the ca-\npacity available for each test language.",
        "971f1cdb-1173-45c5-93de-490cf6b6b08d": "3.3 Analysis\nWe also present additional analysis, to better quan-\ntify when our pre-training helps.\nHow many languages should you pre-train on?\nWe investigate when it is helpful for pre-training\nto include languages other than the targeted lan-\nguage pair that will be used during \ufb01ne tuning. Ta-\nble 5 shows performance on four X-En pairs. Pre-\ntraining on more languages helps most when the\ntarget language monolingual data is limited (e.g.\nEn-My, the size of My is around 0.5%of En).\nIn contrast, when monolingual data is plenti-\nful (De, Ro), pre-training on multiple languages\nslightly hurts the \ufb01nal results (< 1BLEU). In these\ncases, additional languages may reduce the ca-\npacity available for each test language. Addition-\nally, the fact that mBART06 performs similar to\nmBART02 on Ro-En suggests that pre-training\nwith similar languages is particularly helpful.\nHow many pre-training steps are needed? We\nplot Ro-En BLEU score v.s. Pre-training steps in\nFigure 3, where we take the saved checkpoints (ev-\nery 25K steps) and apply the same \ufb01ne-tuning pro-\ncess described in \u00a73.1. Without any pre-training,\nour model over\ufb01ts and performs much worse than\nthe baseline. However, after just 25K steps (5% of\ntraining), both models outperform the best base-\nline. The models keep improving by over 3BLEU\nfor the rest of steps and have not fully con-\nverged after 500K steps. mBART25 is consistently",
        "7f0086c3-a7f9-47c9-8e65-031a3c891042": "Languages De Ro It My En\nSize/GB 66.6 61.4 30.2 1.6 300.8\nmBART02 31.3 38.5 39.7 36.5\nmBART06 - 38.5 39.3 -\nmBART25 30.5 37.7 39.8 36.9\nTable 5: Pretraining Languages on En-X translation.\nThe size refers to the size of monolingual data for X.\nThe size of En is shown as reference. All the pretrained\nmodels were controlled to see the same number of En-\nglish instances during training.\nModelsEn-My Training Cost\n\u2190 \u2192 GPU hours\nRandom (2019) 23.3 34.9 5\n+ BT 32.0 37.7 5 + 300 + 350\nmBART02 29.1 37.8 300\u223c3000 + 40\n+ BT 34.9 39.2 -\nTable 6: Comparison with Back-Translation on My-En\ntranslation using same mono-lingual data. We also esti-\nmate the computational costs for both pre-training and\nback-translation based on Nvidia V100 GPUs.\nslightly worse than mBART02.\nHow does the size of bitexts inference the gain\nfrom pre-training? Tables 2 and 3 show that\npre-training consistently improves for low and\nmedium resource language pairs. To verify this\ntrend, we plot performance for differing sized sub-\nsets of the En-De dataset. More precisely, we take\nthe full En-De corpus ( 28M pairs) and randomly\nsample 10K, 50K, 100K, 500K, 1M, 5M, 10M\ndatasets. We compare performance without pre-\ntraining to the mBART02 results, as shown in Fig-\nure 4. The pre-trained model is able to achieve\nover 20 BLEU with only 10K training examples,\nwhile the baseline system scores 0. Unsurpris-\ningly, increasing the size of bi-text corpus im-\nproves both models. Our pre-trained model con-\nsistently outperforms the baseline models, but the\ngap reduces with increasing amounts of bi-text, es-\npecially after 10M sentence pairs. This result con-\n\ufb01rms our observation in \u00a73.2 that our pre-training\ndoes not help translation in high-resource pairs.\nIs pre-training complementary to BT? Fig-\nure 2 presents that our pre-trained models can\nbe combined with iterative back-translation (BT)\non additional data, however, it is still not a fair\ncomparison. Table 6 shows the results when using\n0 100 200 300 400 500\npretraining steps (K)3435363738Finetuning BLEU\nmBART25\nmBART02\nRandomFigure 3: Fine-tuning curves for Ro-En along with\nPre-training steps . Both mBART25 and mBART02\noutperform the best baseline system after 25K steps.\n104105106107\nBi-text Size (# of sentence pairs)051015202530Finetuning BLEU\n0.04.49.214.722.426.527.430.9\n20.724.224.926.9 27.229.130.231.3\nRandom\nmBART02\nFigure 4: Fine-tuning curves for En-De along with\nsize of bitext. The x-axis is on a log scale.\nsame monolingual data where we use 79M En and\n29M My sentences following Chen et al. (2019).\nWith the same amount of monolingual corpus,\nmBART pre-training achieves the same perfor-\nmance on En\u2192My as BT, while still 3BLEU\nworse on My\u2192En. We suspect BT bene\ufb01ts from\nbigger monolingual data (En). Moreover, combin-\ning mBART02 model with BT, we see further\ngains even with same monolingual data. Besides,\nwe also provide estimated training costs where BT\nhas a longer pipeline involving training a baseline\nsystem (5h), translating monolingual data (300h)\nand formal training (350h). Instead, most of train-\ning costs of mBART lies in the pre-training part\nand can be easily adjusted to be more ef\ufb01cient.",
        "633b793c-11b2-442f-9bf2-93d036e46b37": "same monolingual data where we use 79M En and\n29M My sentences following Chen et al. (2019).\nWith the same amount of monolingual corpus,\nmBART pre-training achieves the same perfor-\nmance on En\u2192My as BT, while still 3BLEU\nworse on My\u2192En. We suspect BT bene\ufb01ts from\nbigger monolingual data (En). Moreover, combin-\ning mBART02 model with BT, we see further\ngains even with same monolingual data. Besides,\nwe also provide estimated training costs where BT\nhas a longer pipeline involving training a baseline\nsystem (5h), translating monolingual data (300h)\nand formal training (350h). Instead, most of train-\ning costs of mBART lies in the pre-training part\nand can be easily adjusted to be more ef\ufb01cient.\n3.4 Generalization to Languages NOT in\nPre-training\nIn this section, we show that mBART can im-\nprove performance even with \ufb01ne tuning for lan-\nguages that did not appear in the pre-training cor-\npora, suggesting that the pre-training has language\nuniversal aspects, especially within the parameters\nlearned at the Transformer layers.",
        "08462388-a0e0-405e-a75b-e4bc6b566a8b": "Monolingual Nl-En En-Nl Ar-En En-Ar Nl-De De-Nl\nRandom None 34.6 (-8.7) 29.3 (-5.5) 27.5 (-10.1) 16.9 (-4.7) 21.3 (-6.4) 20.9 (-5.2)\nmBART02 En Ro 41.4 (-2.9) 34.5 (-0.3) 34.9 (-2.7) 21.2 (-0.4) 26.1 (-1.6) 25.4 (-0.7)\nmBART06 En Ro Cs It Fr Es 43.1 (-0.2) 34.6 (-0.2) 37.3 (-0.3) 21.1 (-0.5) 26.4 (-1.3) 25.3 (-0.8)\nmBART25 All 43.3 34.8 37.6 21.6 27.7 26.1\nTable 7: Generalization to Unseen Languages Language transfer results, \ufb01ne-tuning on language-pairs without\npre-training on them. mBART25 uses all languages during pre-training, while other settings contain at least one\nunseen language pair. For each model, we also show the gap to mBART25 results.\nExperimental Settings We analyze the results\nof three pairs: Nl-En, Ar-En and De-Nl using the\npre-trained mBART25, mBART06 and mBART02\n(EnRo) models. During pre-training, mBART06\nand EnRo Bilingual do not contain Arabic (Ar),\nGerman (De) or Dutch (Nl) data, but all languages\nare in mBART25. Both De and Nl are European\nlanguages and are related to En, Ro and other the\nlanguages in mBART06 pre-training data.\nResults mBART25 uses all languages during\npre-training, but other settings contain at least one\nunseen language. We \ufb01nd large gains from pre-\ntraining on English-Romanian, even when trans-\nlating a distantly related unseen language (Arabic)\nand two unseen languages (German and Dutch).\nThe best results are achieved when pre-training in-\ncludes both test languages, however pre-training\non other languages is surprisingly competitive.\nUnseen Vocabularies Arabic is distantly related\nto the languages in mBART02 and mBART06, and\nits use of a disjoint character set means that it word\nembeddings will be largely untrained. However,\nwe obtain similar improvements on Ar-En pairs to\nthose on Nl-En. This result suggests that the pre-\ntrained Transformer layers learn universal prop-\nerties of language that generalize well even with\nminimal lexical overlap.\nUnseen Source or Target Languages Table 7\nshows different performance when the unseen lan-\nguages are on the source side, target side, or both\nsides. If both sides are unseen, the performance\n(in terms of difference from mBART25) is worse\nthan where at least one language is seen dur-\ning pre-training. Furthermore, although the En-X\npairs perform similarly, mBART06 outperforms\nmBART02 by a margin on X-En pairs. Fine-tuning\nunseen languages on source side is more dif\ufb01cult,\ndeserving more extensive future study.Datasets # Docs # Insts # Sents\nWMT19 En-De 77K 171K 3.7M\nTED15 Zh-En 1.7K 6.5K 0.2M\nTable 8: Statistics for the Document-level Corpus of\nWMT19 En-De and TED15 Zh-En. # of instances is\nthe # of training examples in document model.\n4 Document-level Machine Translation\nWe evaluate mBART on document-level machine\ntranslation tasks, where the goal is to translate seg-\nments of text that contain more than one sentence\n(up to an entire document). During pre-training,\nwe use document fragments of up to 512 tokens,\nallowing the models to learn dependencies be-\ntween sentences. We show that this pre-training\nsigni\ufb01cantly improves document-level translation.\n4.1 Experimental Settings\nDatasets We evaluate performance on two com-\nmon document-level MT datasets: WMT19 En-De\nand TED15 Zh-En (statistics in Table 8).",
        "48e53904-343b-4173-9dda-beadda6160b1": "# of instances is\nthe # of training examples in document model.\n4 Document-level Machine Translation\nWe evaluate mBART on document-level machine\ntranslation tasks, where the goal is to translate seg-\nments of text that contain more than one sentence\n(up to an entire document). During pre-training,\nwe use document fragments of up to 512 tokens,\nallowing the models to learn dependencies be-\ntween sentences. We show that this pre-training\nsigni\ufb01cantly improves document-level translation.\n4.1 Experimental Settings\nDatasets We evaluate performance on two com-\nmon document-level MT datasets: WMT19 En-De\nand TED15 Zh-En (statistics in Table 8). For En-\nDe, we use the document data from WMT19 to\ntrain our model, without any additional sentence-\nlevel data; Zh-En dataset is from the IWSLT 2014\nand 2015 evaluation campaigns (Cettolo et al.,\n2012, 2015). Following Miculicich et al. (2018),\nwe use 2010-2013 TED as the test set.\nPre-processing We use the same pre-processing\nas that in pre-training. For each block, sentences\nare separated by end of sentence symbols (</S>)\nand the entire instance is ended with the speci\ufb01c\nlanguage id (<LID>). The numbers of segmented\ninstances are also shown in Table 8 where on av-\nerage, every document is split into 2-4 instances.\nFine-tuning & Decoding We use the same \ufb01ne-\ntuning scheme as for sentence-level translation\n(\u00a73.1), without using any task-speci\ufb01c techniques\ndeveloped by previous work (Miculicich et al.,",
        "46c05dea-cb33-4fa7-a5ca-aba203f253cc": "(a) Sentence- and Document-level BLEU scores on En-De\nModelRandom mBART25\ns-BLEU d-BLEU s-BLEU d-BLEU\nSent-MT 34.5 35.9 36.4 38.0\nDoc-MT\u00d7 7.7 37.1 38.5(b) Document-level BLEU scores on Zh-En\nModelRandom mBART25 HAN (2018)\nd-BLEU d-BLEU d-BLEU\nSent-MT 22.0 28.4 -\nDoc-MT 3.2 29.6 24.0\nTable 9: Document-Level Machine Translation on En-De and Zh-En. ( \u00d7) The randomly initialized Doc-MT\nmodel cannot produce translations aligned to the original sentences, so only document evaluation is possible.\n2018; Li et al., 2019), such as constrained con-\ntexts or restricted attention. For decoding, we sim-\nply pack the source sentences into blocks, and\ntranslate each instance block autoregressively. The\nmodel does not know how many sentences to gen-\nerate in advance and decoding stops when <LID>\nis predicted. We use beam size 5 by default.\nBaselines & Evaluation We train 4 models: a\ndocument-level (Doc-) MT model (\u00a74.1) and a\ncorresponded sentence-level (Sent-) MT model\n(\u00a73.1) as the baseline, both with and without pre-\ntraining. We use mBART25 as the common pre-\ntrained model for En-De and Zh-En. For En-De,\neven though our mBART25 Doc-MT model de-\ncodes multiple sentences together, the translated\nsentences can be aligned to the source sentences,\nwhich allows us to evaluate BLEU scores both on\nsentence-level (s-BLEU) and document-level (d-\nBLEU)2. For Zh-En, however, we cannot produce\nthe same number of translated sentences as the ref-\nerence due to alignment errors in the test data. We\nonly provide the d-BLEU scores on this direction.\nWe also compare our models with Hierarchi-\ncal Attention Networks (HAN, Miculicich et al.,\n2018) on Zh-En, which is the state-of-the-art non-\npretraining approach for document-level transla-\ntion for this pair. They combine two layers of at-\ntention \u2013 \ufb01rst within and then across sentences.\n4.2 Main Results\nWe show the main results for both En-De and Zh-\nEn are presented in Table 9.\nRandom v.s. Pre-trained The MT models ini-\ntialized with pre-trained weights outperform ran-\ndomly initialized models by large margins, for\nboth sentence-level and document-level training.\nOur mBART25 models (both Sent-MT and Doc-\nMT) also outperform HAN (Miculicich et al.,\n2Standard BLEU scores match n-grams at sentence-level.\nWe also consider document-level where we match n-grams\nover the whole document resulting in a slightly higher score.2018)3, despite the fact that they are not cus-\ntomized for document-level MT in any way.\nSent-MT v.s. Doc-MT For cases (En-De, En-\nZh), the mBART25 Doc-MT models outperform\nthemselves \ufb01ne-tuned at sentence-level by a mar-\ngin, which is completely opposite for models with-\nout pre-training. For both datasets, randomly ini-\ntialized Doc-MT fail to work, resulting in much\nworse results than the sentence-level models. Such\nlarge performance gaps indicate that pre-training\niscritical for document level performance. It is in\ngeneral dif\ufb01cult to collect high quality document-\nlevel data in large quantities, suggesting that pre-\ntraining may be a strong strategy for future work.\nWe also include a sampled example in appendix B.\n5 Unsupervised Machine Translation\nIn addition to supervised machine translation, we\nalso evaluate our model on tasks where no bi-text\nis available for the target language pair. We de\ufb01ne\nthree types of unsupervised translation:\n1. No bi-text of any kind is given. A common so-\nlution is to learn from back-translation (BT)\n(Artetxe et al., 2017; Lample et al., 2018c). We\nshow that mBART provides a simple and effec-\ntive initialize scheme for these methods.\n2.",
        "c871fb00-732a-4d6c-91a9-1d0fe4c4aedb": "Such\nlarge performance gaps indicate that pre-training\niscritical for document level performance. It is in\ngeneral dif\ufb01cult to collect high quality document-\nlevel data in large quantities, suggesting that pre-\ntraining may be a strong strategy for future work.\nWe also include a sampled example in appendix B.\n5 Unsupervised Machine Translation\nIn addition to supervised machine translation, we\nalso evaluate our model on tasks where no bi-text\nis available for the target language pair. We de\ufb01ne\nthree types of unsupervised translation:\n1. No bi-text of any kind is given. A common so-\nlution is to learn from back-translation (BT)\n(Artetxe et al., 2017; Lample et al., 2018c). We\nshow that mBART provides a simple and effec-\ntive initialize scheme for these methods.\n2. No bi-text for the target pair is available, but\nthe target languages both appear in bi-text cor-\npora for other language pairs. Previous work\nhas shown that zero-shot transfer is possible via\nmassively multi-lingual MT (Johnson et al.,\n2017; Gu et al., 2019) or distillation through\npivoting (Chen et al., 2017). We limit our fo-\ncus to building MT models for single language\npairs, and leave multi-lingual pre-training for\nmulti-lingual MT to future work.\n3. No bi-text for the target pair is available, but\nthere is bi-text for translating from some other\n3d-BLEU is recomputed from the provided system output.",
        "9b3d2363-20ff-4193-b532-3b92e29e5223": "mBARTGenerated En TextMonolingual Ne TextmBARTDecodeMLE lossInputInputmBARTGenerated Ne TextMonolingual En TextmBARTDecodeMLE lossInputInputmBARTParallel Hi TextParallel En TextmBARTDecodeMLE lossInputTransfer (no train)Ne TextInputGenerated En Text\n(a)(b)Figure 5: Illustrated frameworks for unsupervised machine translation via (a) back-translation (b) language transfer\nwhere Ne-En is used as an example. For both cases, we initialize from multilingual pre-training (e.g. mBART25).\nlanguage into the target language. This is a new\nevaluation regime, where we will show that\nmBART supports effective transfer, even if the\nsource language has no bi-text of any form.\nIn this section, we demonstrate the effectiveness\nof multilingual pre-training in unsupervised ma-\nchine translation via (1) back-translation ( \u00a75.1)\nand (3) language transfer (\u00a75.2). An illustration of\nboth approaches are presented in Figure 5.\n5.1 Unsupervised Machine Translation via\nBack-Translation\nDatasets We evaluate our pre-trained models on\nboth similar (En-De, En-Ro) and dissimilar pairs\n(En-Ne, En-Si), which are determined by measur-\ning the subword units that are shared between the\nsource and target languages. We use the same test\nsets as the supervised benchmarks \u00a73.1, and di-\nrectly use the pre-training data (CC25) for back-\ntranslation to avoid introducing new information.\nLearning Following the same procedure de-\nscribed in Lample et al. (2018c); Lample and\nConneau (2019), we \ufb01rst initialize the transla-\ntion model with the pre-trained weights, and then\nlearn to predict the monolingual sentences condi-\ntioned on source sentences generated by on-the-\n\ufb02y back-translation (BT). Lample and Conneau\n(2019) only pre-train an encoder, so perform addi-\ntional de-noising training to learn a seq2seq model\n\u2013 a step which is unnecessary for mBART\u2019s pre-\ntrained seq2seq model. However, we do constrain\nmBART to only generating tokens in target lan-\nguage4for the \ufb01rst 1000 steps of on-the-\ufb02y BT, to\navoid it simply copying the source text.\nResults Table 10 shows the unsupervised trans-\nlation results compared with non-pretrained mod-\n4We mask out the output probability of predicting tokens\nwhich appear less than 1%in the target monolingual corpus.els, as well as models with existing pre-training\nmethods. Our models achieve large gains over\nnon-pretrained models for all directions, and out-\nperform XLM signi\ufb01cantly for dissimilar pairs\n(En-Ne, En-Si) where the existing approaches\ncompletely fail. For similar pairs, our model also\nperforms well against XLM and MASS, with the\nbest numbers for En-X pairs.\n5.2 Unsupervised Machine Translation via\nLanguage Transfer\nThe second case of unsupervised machine transla-\ntion assumes the target language appears in a bi-\ntext corpus with some other source language.\nDatasets We only consider X \u2192En translation,\nand choose the bitexts of 12 language pairs from\n\u00a73.1, covering Indic languages (Ne, Hi, Si, Gu),\nEuropean languages (Ro, It, Cs, Nl), East Asian\nlanguages (Zh, Ja, Ko) and Arabic languages (Ar).\nResults As illustrated in Figure 5 (b), we take\nthe pre-trained mBART25 model and \ufb01netune on\neach language pair, and then directly apply them\nto the rest of pairs, as seen in Table 11. We also\npresent the direct \ufb01ne-tuning performance (\u00a73) on\nthe diagonal, for reference. We can always ob-\ntain reasonable transferring scores at all pairs over\ndifferent \ufb01ne-tuned models except from Gu-En\nwhere the supervised model completely fails ( 0.3\nBLEU). In some cases, we can achieve similar\n(Cs-En) or even much better (Ne-En, Gu-En) re-\nsults compared to the supervised results.\nAs a comparison, we also apply the same proce-\ndure on randomly initialized models without pre-\ntraining, which always ends up with \u22480BLEU.\nThis indicates that multilingual pre-training is\nessential and produces universal representations\nacross languages, so that once the model learns\nto translate one language to En, it learns to trans-",
        "efb4ae24-c81c-4d82-ae9e-e08d6288d484": "ModelSimilar Pairs Dissimilar Pairs\nEn-De En-Ro En-Ne En-Si\n\u2190 \u2192 \u2190 \u2192 \u2190 \u2192 \u2190 \u2192\nRandom 21.0 17.2 19.4 21.2 0.0 0.0 0.0 0.0\nXLM (2019) 34.3 26.4 31.8 33.3 0.5 0.1 0.1 0.1\nMASS (2019) 35.2 28.3 33.1 35.2 - - - -\nmBART 34.0 29.8 30.5 35.0 10.0 4.4 8.2 3.9\nTable 10: Unsupervised MT via Back-Translation . En-De, En-Ro are initialized by mBART02, while En-Ne,\nEn-Si are initialized by mBART25. Our models are trained on monolingual data used in pre-training.\nFine-tuning Languages\nZh Ja Ko Cs Ro Nl It Ar Hi Ne Si Gu\nDomain News TED TED News News TED TED TED News Wiki Wiki WikiTesting LanguagesZh 23.7 8.8 9.2 2.8 7.8 7.0 6.8 6.2 7.2 4.2 5.9 0.0\nJa 9.9 19.1 12.2 0.9 4.8 6.4 5.1 5.6 4.7 4.2 6.5 0.0\nKo 5.8 16.9 24.6 5.7 8.5 9.5 9.1 8.7 9.6 8.8 11.1 0.0\nCs 9.3 15.1 17.2 21.6 19.5 17.0 16.7 16.9 13.2 15.1 16.4 0.0\nRo 16.2 18.7 17.9 23.0 37.8 22.3 21.6 22.6 16.4 18.5 22.1 0.0\nNl 14.4 30.4 32.3 21.2 27.0 43.3 34.1 31.0 24.6 23.3 27.3 0.0\nIt 16.9 25.8 27.8 17.1 23.4 30.2 39.8 30.6 20.1 18.5 23.2 0.0\nAr 5.8 15.5 12.8 12.7 12.0 14.7 14.7 37.6 11.6 13.0 16.7 0.0\nHi 3.2 10.1 9.9 5.8 6.7 6.1 5.0 7.6 23.5 14.5 13.0 0.0\nNe 2.1 6.7 6.5 5.0 4.3 3.0 2.2 5.2 17.9 14.5 10.8 0.0\nSi 5.0 5.7 3.8 3.8 1.3 0.9 0.5 3.5 8.1 8.9 13.7 0.0\nGu 8.2 8.5 4.7 5.4 3.5 2.1 0.0 6.2 13.8 13.5 12.8 0.3\nTable 11: Unsupervised MT via Language Transfer on X-En translations. The model \ufb01ne-tuned on one language\npair is directly tested on another. We use gray color to show the direct \ufb01ne-tuning results, and lightgray color to\nshow language transfer within similar language groups. We bold the highest transferring score for each pair.",
        "35bee546-5ec5-4e6c-9d43-ec2ce1cf3c8c": "The model \ufb01ne-tuned on one language\npair is directly tested on another. We use gray color to show the direct \ufb01ne-tuning results, and lightgray color to\nshow language transfer within similar language groups. We bold the highest transferring score for each pair.\nPairs BT Transfer Combined\nRo\u2192En 30.5 Cs\u2192En 23.0 33.9\nNe\u2192En 10.0 Hi\u2192En 18.9 22.1\nZh\u2192En 11.3 Ko\u2192En 9.2 15.0\nNl\u2192En 28.5 It\u2192En 34.1 35.4\nTable 12: Back-Translation v.s. Language Transfer\nfor Unsupervised MT . We present the best transfer-\nring scores together with the pairs transferred from.\nlate all languages with similar representations. We\nalso present three examples of language transfer-\nring between Zh, Ja and Ko in appendix B.\nWhen is language transfer useful? Table 11\nalso shows mixed results at each pair. First, for\nmost pairs, language transfer works better when\n\ufb01ne-tuning is also conducted in the same language\nfamily, especially between Indic languages (Hi,\nNe, Gu). However, signi\ufb01cant vocabulary sharing\nis not required for effective transfer. For instance,\nZh-En and It-En achieve the best transfer learning\nresults on Ko-En and Ar-En, respectively. How-ever, the vocabulary overlapping (even character\noverlapping) between Zh and Ko, It and Ar is low.\nw/ Back-Translation We also present the com-\nparison on 4 pairs of unsupervised MT with back-\ntranslation (BT) v.s. language transfer in Table 12.\nThe results are also mixed. If there exists high\nquality (similar languages) bi-text data, or trans-\nlating between dissimilar pairs, language transfer\nis able to beat the conventional methods with BT.\nFurthermore, we also show promising results for\ncombining these two techniques. In such cases, we\nstart from the best transferred model and apply (it-\nerative) BT on the same monolingual corpus used\nin pre-training. Table 12 presents the results with 1\niteration of BT. For all pairs, we see improvements\nby combining both techniques.\n6 Related Work\nPre-training for Text Generation This work\ninherits from the recent success brought by self-\nsupervised pre-training for NLP applications (Pe-",
        "3d751562-2ef4-4b50-a37f-f1b223f99a6e": "ters et al., 2018; Radford et al., 2018; Devlin et al.,\n2019; Yang et al., 2019; Liu et al., 2019), espe-\ncially for text generation tasks (Radford et al.,\n2019; Song et al., 2019; Dong et al., 2019; Raf-\nfel et al., 2019; Lewis et al., 2019) where dif-\nferent self-supervised objectives are designed for\ntraining big neural models on enormous unlabeled\ntext corpora The pre-trained models are usually\nused as the initialization for \ufb01ne-tuning variant\ndownstream tasks such as controllable language\nmodeling (Shirish Keskar et al., 2019), machine\ntranslation (Song et al., 2019), summarization (Liu\nand Lapata, 2019) and dialogue generation (Zhang\net al., 2019). In contrast to most prior work, we\nfocus on a deep exploration of applying denoising\npre-training for various translation applications.\nMultilinguality in NLP tasks This work is also\nrelated to the continual trend of multilingual lan-\nguage learning, including aligning multilingual\nword embeddings (Mikolov et al., 2013; Chen and\nCardie, 2018; Lample et al., 2018b) into universal\nspace, and learning cross-lingual models (Wada\nand Iwata, 2018; Lample and Conneau, 2019;\nConneau et al., 2019) to exploit shared represen-\ntations across languages.\nFor machine translation, the most relevant \ufb01eld\nismultilingual translation (Firat et al., 2016;\nVi\u00e9gas et al., 2016; Aharoni et al., 2019; Arivazha-\ngan et al., 2019) where the ultimate goal is to\njointly train one translation model that translates\nmultiple language directions at the same time, and\nshares representations to improve the translation\nperformance on low-resource languages (Gu et al.,\n2018). In this paper, we mainly focus on multilin-\ngualism in the pre-training stage and \ufb01ne-tune the\nlearned model in the standard bi-lingual scenario.\nCompared to multilingual translation, we do not\nrequire parallel data across multiple languages but\nthe targeted direction, which potentially improves\nthe scalability to low-resource languages and spe-\nci\ufb01c domains. Moreover, multilingual pre-training\nis unlikely to suffer the interference problems be-\ntween dissimilar languages, which is typical for\nregular multilingual translation models.\nDocument Translation As one of the key appli-\ncations, this work also links to previous efforts for\nincorporating document-level contexts into neu-\nral machine translation (Wang et al., 2017; Jean\net al., 2017; Tiedemann and Scherrer, 2017; Mi-\nculicich et al., 2018; Tu et al., 2018). Li et al.(2019) is the most relevant work which also uti-\nlized pre-trained encoder (BERT) for handling\nlonger context. However, none of these works had\nshown positive results on pure Seq2Seq models\nat document-level, which involved task-speci\ufb01c\ntechniques, and usually only worked on sentence-\nlevel translation with a constrained range of con-\ntext. To the extent of our knowledge, our mul-\ntilingual pre-trained model is the \ufb01rst-of-its-kind\nwork that shows improved results on document-\nlevel translation with standard Seq2Seq learning.\nUnsupervised Translation This work also sum-\nmarizes the previous efforts of learning to translate\nbetween languages without a direct parallel cor-\npus, and re-de\ufb01nes them as unsupervised machine\ntranslation with three categories where in this\nwork, we only focus on applications to the \ufb01rst and\nthe third kinds (\u00a75). When no parallel corpus of\nany kind is available, Artetxe et al. (2017); Lample\net al. (2018a,c) proposed to jointly learn denois-\ning auto-encoder and back-translation from both\ndirections, which, however, required good initial-\nization and only worked well on similar language\npairs; Wu et al. (2019a) replaced back-translation\nwith retrieved similar sentences from target mono-\nlingual data; Wu et al. (2019b) solves the problem\nby mining sentences from Wikipedia and use them\nas weakly supervised translation pairs.",
        "3b3dff44-8861-4a4e-a8a7-7d90e02a41ba": "When no parallel corpus of\nany kind is available, Artetxe et al. (2017); Lample\net al. (2018a,c) proposed to jointly learn denois-\ning auto-encoder and back-translation from both\ndirections, which, however, required good initial-\nization and only worked well on similar language\npairs; Wu et al. (2019a) replaced back-translation\nwith retrieved similar sentences from target mono-\nlingual data; Wu et al. (2019b) solves the problem\nby mining sentences from Wikipedia and use them\nas weakly supervised translation pairs. Similar to\nLample and Conneau (2019); Song et al. (2019),\nwe follow the \ufb01rst approach and treat our pre-\ntrained model as the initialization step. Besides,\nwe investigate unsupervised translation using lan-\nguage transfer, which is similar to Pourdamghani\net al. (2019) where the authors generate transla-\ntionese of the source language and train a sys-\ntem on high-resource languages to correct these\nintermediate utterances. It is also closely related\nto Conneau et al. (2018); Artetxe et al. (2019) for\ncross-lingual representation learning.\n7 Conclusion\nWe demonstrate that multilingual de-noising pre-\ntraining is able to signi\ufb01cantly improve both su-\npervised and unsupervised machine translation at\nboth the sentence level and document level. We\nanalyze when and how pre-training is most effec-\ntive and can be combined with other approaches\nsuch as back-translation. Our results also show the\ntransfer learning ability of the learned representa-\ntions from multilingual pre-training.\nIn future work, we will scale-up the current pre-",
        "c5b27e27-3d85-429c-859f-44e0197c7185": "training to more languages, e.g., an mBART100\nmodel. The size of our model makes it expensive\nto deploy in production \u2013 future work will explore\npre-training more ef\ufb01cient models.\n8 Acknowledgements\nWe thank Marc\u2019Aurelio Ranzato, Guillaume Lam-\nple, Alexis Conneau, and Michael Auli for shar-\ning their expertise on low-resource and unsuper-\nvised machine translation, Peng-Jen Chen, Jiajun\nShen for details about FloRes and WAT datasets.\nWe also thank our colleagues at FAIR and FAIAR\nfor valuable feedback.\nReferences\nRoee Aharoni, Melvin Johnson, and Orhan Firat.\n2019. Massively multilingual neural machine\ntranslation. In Proceedings of the 2019 Con-\nference of the North American Chapter of the\nAssociation for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long\nand Short Papers) , pages 3874\u20133884, Min-\nneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nNaveen Arivazhagan, Ankur Bapna, Orhan Fi-\nrat, Dmitry Lepikhin, Melvin Johnson, Maxim\nKrikun, Mia Xu Chen, Yuan Cao, George\nFoster, Colin Cherry, Wolfgang Macherey,\nZhifeng Chen, and Yonghui Wu. 2019. Mas-\nsively multilingual neural machine translation\nin the wild: Findings and challenges. CoRR ,\nabs/1907.05019.\nMikel Artetxe, Gorka Labaka, Eneko Agirre,\nand Kyunghyun Cho. 2017. Unsupervised\nneural machine translation. arXiv preprint\narXiv:1710.11041 .\nMikel Artetxe, Sebastian Ruder, and Dani Yo-\ngatama. 2019. On the cross-lingual transferabil-\nity of monolingual representations.\nMauro Cettolo, Christian Girardi, and Marcello\nFederico. 2012. Wit3: Web inventory of tran-\nscribed and translated talks. In Conference of\nEuropean Association for Machine Translation ,\npages 261\u2013268.\nMauro Cettolo, Niehues Jan, St\u00fcker Sebastian,\nLuisa Bentivogli, Roldano Cattoni, and Mar-\ncello Federico. 2015. The iwslt 2015 evalua-tion campaign. In International Workshop on\nSpoken Language Translation .\nPeng-Jen Chen, Jiajun Shen, Matt Le, Vishrav\nChaudhary, Ahmed El-Kishky, Guillaume Wen-\nzek, Myle Ott, and Marc\u2019Aurelio Ranzato.\n2019. Facebook ai\u2019s wat19 myanmar-english\ntranslation task submission. arXiv preprint\narXiv:1910.06848 .\nXilun Chen and Claire Cardie. 2018. Unsuper-\nvised multilingual word embeddings. In Pro-\nceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing ,\npages 261\u2013270, Brussels, Belgium. Association\nfor Computational Linguistics.\nYun Chen, Yang Liu, Yong Cheng, and Victor OK\nLi. 2017. A teacher-student framework for\nzero-resource neural machine translation. In\nProceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Vol-\nume 1: Long Papers) , pages 1925\u20131935.\nAlexis Conneau, Kartikay Khandelwal, Naman\nGoyal, Vishrav Chaudhary, Guillaume Wen-\nzek, Francisco Guzm\u00e1n, Edouard Grave, Myle\nOtt, Luke Zettlemoyer, and Veselin Stoyanov.\n2019. Unsupervised cross-lingual represen-\ntation learning at scale. arXiv preprint\narXiv:1911.02116 .\nAlexis Conneau, Ruty Rinott, Guillaume Lample,\nAdina Williams, Samuel R. Bowman, Holger\nSchwenk, and Veselin Stoyanov. 2018. Xnli:\nEvaluating cross-lingual sentence representa-\ntions. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language\nProcessing . Association for Computational Lin-\nguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019.",
        "31c1b209-ed10-431a-ba0f-10cdc8c5a620": "2019. Unsupervised cross-lingual represen-\ntation learning at scale. arXiv preprint\narXiv:1911.02116 .\nAlexis Conneau, Ruty Rinott, Guillaume Lample,\nAdina Williams, Samuel R. Bowman, Holger\nSchwenk, and Veselin Stoyanov. 2018. Xnli:\nEvaluating cross-lingual sentence representa-\ntions. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language\nProcessing . Association for Computational Lin-\nguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training\nof deep bidirectional transformers for language\nunderstanding. In North American Association\nfor Computational Linguistics (NAACL) .\nChenchen Ding, Hnin Thu Zar Aye, Win Pa Pa,\nKhin Thandar Nwet, Khin Mar Soe, Masao\nUtiyama, and Eiichiro Sumita. 2019. Towards\nBurmese (Myanmar) morphological analysis:\nSyllable-based tokenization and part-of-speech\ntagging. ACM Transactions on Asian and\nLow-Resource Language Information Process-\ning (TALLIP) , 19(1):5.",
        "4feaef2b-cfc3-4602-854c-510e9f3d4b55": "Chenchen Ding, Masao Utiyama, and Eiichiro\nSumita. 2018. NOV A: A feasible and \ufb02exi-\nble annotation system for joint tokenization and\npart-of-speech tagging. ACM Transactions on\nAsian and Low-Resource Language Informa-\ntion Processing (TALLIP) , 18(2):17.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei,\nXiaodong Liu, Yu Wang, Jianfeng Gao, Ming\nZhou, and Hsiao-Wuen Hon. 2019. Uni\ufb01ed lan-\nguage model pre-training for natural language\nunderstanding and generation. arXiv preprint\narXiv:1905.03197 .\nSergey Edunov, Alexei Baevski, and Michael Auli.\n2019. Pre-trained language model representa-\ntions for language generation. arXiv preprint\narXiv:1903.09722 .\nOrhan Firat, Kyunghyun Cho, and Yoshua Bengio.\n2016. Multi-way, multilingual neural machine\ntranslation with a shared attention mechanism.\nInNAACL .\nJiatao Gu, Hany Hassan, Jacob Devlin, and Vic-\ntor O.K. Li. 2018. Universal neural machine\ntranslation for extremely low resource lan-\nguages. In Proceedings of the 2018 Conference\nof the North American Chapter of the Asso-\nciation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Pa-\npers) , pages 344\u2013354, New Orleans, Louisiana.\nAssociation for Computational Linguistics.\nJiatao Gu, Yong Wang, Kyunghyun Cho, and Vic-\ntor OK Li. 2019. Improved zero-shot neural\nmachine translation via ignoring spurious cor-\nrelations. arXiv preprint arXiv:1906.01181 .\nFrancisco Guzm\u00e1n, Peng-Jen Chen, Myle Ott,\nJuan Pino, Guillaume Lample, Philipp Koehn,\nVishrav Chaudhary, and Marc\u2019Aurelio Ran-\nzato. 2019. The FLORES evaluation datasets\nfor low-resource machine translation: Nepali\u2013\nEnglish and Sinhala\u2013English. In Proceedings\nof the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the\n9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP) ,\npages 6097\u20136110, Hong Kong, China. Associ-\nation for Computational Linguistics.\nS\u00e9bastien Jean, Stanislas Lauly, Orhan Firat, and\nKyunghyun Cho. 2017. Does neural machinetranslation bene\ufb01t from larger context? CoRR ,\nabs/1704.05135.\nMelvin Johnson, Mike Schuster, Quoc V Le,\nMaxim Krikun, Yonghui Wu, Zhifeng Chen,\nNikhil Thorat, Fernanda Vi\u00e9gas, Martin Wat-\ntenberg, Greg Corrado, et al. 2017. Google\u2019s\nmultilingual neural machine translation system:\nEnabling zero-shot translation. Transactions of\nthe Association for Computational Linguistics ,\n5:339\u2013351.\nTaku Kudo and John Richardson. 2018. Senten-\ncePiece: A simple and language independent\nsubword tokenizer and detokenizer for neural\ntext processing. In Proceedings of the 2018\nConference on Empirical Methods in Natural\nLanguage Processing: System Demonstrations ,\npages 66\u201371, Brussels, Belgium. Association\nfor Computational Linguistics.\nAnoop Kunchukuttan, Pratik Mehta, and Pushpak\nBhattacharyya. 2017. The IIT bombay english-\nhindi parallel corpus. CoRR , abs/1710.02855.\nGuillaume Lample and Alexis Conneau. 2019.\nCross-lingual language model pretraining.\narXiv preprint arXiv:1901.07291 .\nGuillaume Lample, Alexis Conneau, Ludovic De-\nnoyer, and Marc\u2019Aurelio Ranzato. 2018a. Un-\nsupervised machine translation using monolin-\ngual corpora only. In International Conference\non Learning Representations .\nGuillaume Lample, Alexis Conneau,\nMarc\u2019Aurelio Ranzato, Ludovic Denoyer,\nand Herv\u00e9 J\u00e9gou. 2018b. Word transla-\ntion without parallel data.",
        "17a1069e-f779-44b9-aecf-b80dc7270f15": "Anoop Kunchukuttan, Pratik Mehta, and Pushpak\nBhattacharyya. 2017. The IIT bombay english-\nhindi parallel corpus. CoRR , abs/1710.02855.\nGuillaume Lample and Alexis Conneau. 2019.\nCross-lingual language model pretraining.\narXiv preprint arXiv:1901.07291 .\nGuillaume Lample, Alexis Conneau, Ludovic De-\nnoyer, and Marc\u2019Aurelio Ranzato. 2018a. Un-\nsupervised machine translation using monolin-\ngual corpora only. In International Conference\non Learning Representations .\nGuillaume Lample, Alexis Conneau,\nMarc\u2019Aurelio Ranzato, Ludovic Denoyer,\nand Herv\u00e9 J\u00e9gou. 2018b. Word transla-\ntion without parallel data. In International\nConference on Learning Representations .\nGuillaume Lample, Myle Ott, Alexis Conneau,\nLudovic Denoyer, and Marc\u2019Aurelio Ranzato.\n2018c. Phrase-based & neural unsuper-\nvised machine translation. arXiv preprint\narXiv:1804.07755 .\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2019. Bart: Denoising sequence-to-sequence\npre-training for natural language generation,\ntranslation, and comprehension. arXiv preprint\narXiv:1910.13461 .",
        "27aaef43-9da6-4e44-90bc-70dee239a8e8": "Liangyou Li, Xin Jiang, and Qun Liu. 2019.\nPretrained language models for document-level\nneural machine translation. arXiv preprint\narXiv:1911.03110 .\nYang Liu and Mirella Lapata. 2019. Text sum-\nmarization with pretrained encoders. arXiv\npreprint arXiv:1908.08345 .\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei\nDu, Mandar Joshi, Danqi Chen, Omer Levy,\nMike Lewis, Luke Zettlemoyer, and Veselin\nStoyanov. 2019. Roberta: A robustly opti-\nmized bert pretraining approach. arXiv preprint\narXiv:1907.11692 .\nLesly Miculicich, Dhananjay Ram, Nikolaos Pap-\npas, and James Henderson. 2018. Document-\nlevel neural machine translation with hierarchi-\ncal attention networks. In Proceedings of the\n2018 Conference on Empirical Methods in Nat-\nural Language Processing , pages 2947\u20132954,\nBrussels, Belgium. Association for Computa-\ntional Linguistics.\nTomas Mikolov, Quoc V . Le, and Ilya Sutskever.\n2013. Exploiting similarities among languages\nfor machine translation. CoRR , abs/1309.4168.\nMyle Ott, Sergey Edunov, Alexei Baevski, An-\ngela Fan, Sam Gross, Nathan Ng, David Grang-\nier, and Michael Auli. 2019. FAIRSEQ : A fast,\nextensible toolkit for sequence modeling. In\nNorth American Association for Computational\nLinguistics (NAACL): System Demonstrations .\nKishore Papineni, Salim Roukos, Todd Ward, and\nWei-Jing Zhu. 2002. Bleu: a method for au-\ntomatic evaluation of machine translation. In\nProceedings of the 40th annual meeting on as-\nsociation for computational linguistics , pages\n311\u2013318. Association for Computational Lin-\nguistics.\nMatthew Peters, Mark Neumann, Mohit Iyyer,\nMatt Gardner, Christopher Clark, Kenton Lee,\nand Luke Zettlemoyer. 2018. Deep contextu-\nalized word representations. In North Ameri-\ncan Association for Computational Linguistics\n(NAACL) .\nMatt Post. 2018. A call for clarity in reporting\nBLEU scores. In Proceedings of the Third Con-\nference on Machine Translation: Research Pa-pers, pages 186\u2013191, Belgium, Brussels. Asso-\nciation for Computational Linguistics.\nNima Pourdamghani, Nada Aldarrab, Marjan\nGhazvininejad, Kevin Knight, and Jonathan\nMay. 2019. Translating translationese: A two-\nstep approach to unsupervised machine transla-\ntion. In ACL.\nAlec Radford, Karthik Narasimhan, Time Sali-\nmans, and Ilya Sutskever. 2018. Improving lan-\nguage understanding with unsupervised learn-\ning. Technical report, OpenAI.\nAlec Radford, Jeffrey Wu, Rewon Child, David\nLuan, Dario Amodei, and Ilya Sutskever. 2019.\nLanguage models are unsupervised multitask\nlearners. Technical report, OpenAI.\nColin Raffel, Noam Shazeer, Adam Roberts,\nKatherine Lee, Sharan Narang, Michael\nMatena, Yanqi Zhou, Wei Li, and Peter J Liu.\n2019. Exploring the limits of transfer learning\nwith a uni\ufb01ed text-to-text transformer. arXiv\npreprint arXiv:1910.10683 .\nRico Sennrich, Barry Haddow, and Alexandra\nBirch. 2016a. Edinburgh neural machine trans-\nlation systems for wmt 16. In Proceedings of\nthe First Conference on Machine Translation:\nVolume 2, Shared Task Papers , pages 371\u2013376.\nRico Sennrich, Barry Haddow, and Alexandra\nBirch. 2016b. Improving neural machine trans-\nlation models with monolingual data. In Pro-\nceedings of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics (Vol-\nume 1: Long Papers) , pages 86\u201396, Berlin, Ger-\nmany.",
        "3073d447-a4e3-489f-8913-107e29cb186d": "2019. Exploring the limits of transfer learning\nwith a uni\ufb01ed text-to-text transformer. arXiv\npreprint arXiv:1910.10683 .\nRico Sennrich, Barry Haddow, and Alexandra\nBirch. 2016a. Edinburgh neural machine trans-\nlation systems for wmt 16. In Proceedings of\nthe First Conference on Machine Translation:\nVolume 2, Shared Task Papers , pages 371\u2013376.\nRico Sennrich, Barry Haddow, and Alexandra\nBirch. 2016b. Improving neural machine trans-\nlation models with monolingual data. In Pro-\nceedings of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics (Vol-\nume 1: Long Papers) , pages 86\u201396, Berlin, Ger-\nmany. Association for Computational Linguis-\ntics.\nNitish Shirish Keskar, Bryan McCann, Lav R\nVarshney, Caiming Xiong, and Richard Socher.\n2019. Ctrl: A conditional transformer lan-\nguage model for controllable generation. arXiv\npreprint arXiv:1909.05858 .\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and\nTie-Yan Liu. 2019. MASS: Masked sequence\nto sequence pre-training for language genera-\ntion. In International Conference on Machine\nLearning (ICML) .\nJ\u00f6rg Tiedemann and Yves Scherrer. 2017. Neu-\nral machine translation with extended context.",
        "954f2c99-3805-4ccc-974d-50e6dd2451b3": "InProceedings of the Third Workshop on Dis-\ncourse in Machine Translation , pages 82\u201392,\nCopenhagen, Denmark. Association for Com-\nputational Linguistics.\nZhaopeng Tu, Yang Liu, Shuming Shi, and Tong\nZhang. 2018. Learning to remember translation\nhistory with a continuous cache. Transactions\nof the Association for Computational Linguis-\ntics, 6:407\u2013420.\nAshish Vaswani, Noam Shazeer, Niki Parmar,\nJakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. 2017. At-\ntention is all you need. In Advances in neural\ninformation processing systems .\nFernanda Vi\u00e9gas, Greg Corrado, Jeffrey Dean,\nMacduff Hughes, Martin Wattenberg, Maxim\nKrikun, Melvin Johnson, Mike Schuster, Nikhil\nThorat, Quoc V Le, et al. 2016. Google\u2019s multi-\nlingual neural machine translation system: En-\nabling zero-shot translation.\nTakashi Wada and Tomoharu Iwata. 2018. Un-\nsupervised cross-lingual word embedding by\nmultilingual neural language models. CoRR ,\nabs/1809.02306.\nLongyue Wang, Zhaopeng Tu, Andy Way, and\nQun Liu. 2017. Exploiting cross-sentence con-\ntext for neural machine translation. In Pro-\nceedings of the 2017 Conference on Empiri-\ncal Methods in Natural Language Processing ,\npages 2826\u20132831, Copenhagen, Denmark. As-\nsociation for Computational Linguistics.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis\nConneau, Vishrav Chaudhary, Francisco Guz-\nman, Armand Joulin, and Edouard Grave. 2019.\nCcnet: Extracting high quality monolingual\ndatasets from web crawl data. arXiv preprint\narXiv:1911.00359 .\nJiawei Wu, Xin Wang, and William Yang Wang.\n2019a. Extract and edit: An alternative to back-\ntranslation for unsupervised neural machine\ntranslation. arXiv preprint arXiv:1904.02331 .\nLijun Wu, Jinhua Zhu, Di He, Fei Gao, Xu Tan,\nTao Qin, and Tie-Yan Liu. 2019b. Machine\ntranslation with weakly paired bilingual docu-\nments.Zhilin Yang, Zihang Dai, Yiming Yang, Jaime\nCarbonell, Ruslan Salakhutdinov, and Quoc V\nLe. 2019. Xlnet: Generalized autoregressive\npretraining for language understanding. arXiv\npreprint arXiv:1906.08237 .\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun\nChen, Chris Brockett, Xiang Gao, Jianfeng\nGao, Jingjing Liu, and Bill Dolan. 2019. Di-\nalogpt: Large-scale generative pre-training for\nconversational response generation.\nA Evaluation Details\nFor all our tasks, we use BLEU scores (Papineni\net al., 2002) as the automatic metric to evaluate\nthe translation performance. Normally, we com-\npute the BLEU scores over tokenized text for both\nsystem outputs and the references, and we apply\nlanguage-wise tokenization after over the trans-\nlation. Note that, since we directly work on raw\ntexts, we automatically get de-tokenized output af-\nter recovering sentence-piece subwords. Follow-\ning the literature, the instructions of language-wise\ntokenization are as follows:\n\u2022Gu, Ne, Si, Hi : We use Indic-NLP Library5to\ntokenize the Indic language outputs.\n\u2022Ja: We use KyTea6to segment Japanese texts.\n\u2022Ko: We use Mecab-Ko7and its default dictio-\nnary to segment the Korean texts\n\u2022Ar: We apply QCRI Arabic Normalizer8over\nthe Arabic texts.\n\u2022My: We use the of\ufb01cial segmentation tool pro-\nvided by Ding et al. (2019) for Burmese.\n\u2022Ro: Following Sennrich et al. (2016a), we ap-\nply Moses tokenization and special normaliza-\ntion for Romanian texts9.",
        "02042ab9-7641-4b93-8eb4-d5e7ea74014a": "Note that, since we directly work on raw\ntexts, we automatically get de-tokenized output af-\nter recovering sentence-piece subwords. Follow-\ning the literature, the instructions of language-wise\ntokenization are as follows:\n\u2022Gu, Ne, Si, Hi : We use Indic-NLP Library5to\ntokenize the Indic language outputs.\n\u2022Ja: We use KyTea6to segment Japanese texts.\n\u2022Ko: We use Mecab-Ko7and its default dictio-\nnary to segment the Korean texts\n\u2022Ar: We apply QCRI Arabic Normalizer8over\nthe Arabic texts.\n\u2022My: We use the of\ufb01cial segmentation tool pro-\nvided by Ding et al. (2019) for Burmese.\n\u2022Ro: Following Sennrich et al. (2016a), we ap-\nply Moses tokenization and special normaliza-\ntion for Romanian texts9.\n\u2022Zh: We use the of\ufb01cial sacreBleu (Post, 2018)10\nChinese tokenizer (\u2013tok zh).\nFor other languages that are not listed above, we\ncompute BLEU scores with sacreBLEU with DE-\nFAULT tokenization.\nB Translation Examples\n5https://anoopkunchukuttan.github.io/indic_nlp_library/\n6http://www.phontron.com/kytea/\n7http://konlpy.org/en/v0.3.0/install/\n8http://alt.qcri.org/tools/arabic-normalizer/\n9https://github.com/rsennrich/wmt16-script\n10https://github.com/mjpost/sacreBLEU",
        "b7794895-6a00-413f-af52-0bd007dfdd20": "?\n?\n?\n?\n?\n?\n?? \n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n? \n? \n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n? \n?\n?\n?\n?\n?\n?\n?\n?\n?\n? \n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?? \n?\n?10\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?? \n?\n?\n?\n?\n?\n?\n?\n?\n?\n?? \n?\n?\n?\n?\n?? \n?\n?\n?\n?\n?\n?\n?\n??\n?\n?\n?\n?? \n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?? \n?\n?\n?\n?\n?\n?\n? \n?\n?\n?\n?\n?\n?200\n?\n?? \n?\n?\n?\n?\n?\n?\n?\n? \n?\n?\n?\n?\n?\n?\n? \n?\n?\n?\n?\n?\n?\n?\n?? \n?\n?\n?\n?\n?\n?\n?? \n?\n?\n?\n?\n?\n? \n?\n?\n?\n?\n?\n?\n?\n?\n? \n?\n?\n?\n?\n?\n?\n?\n?\n?? \n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?? \n?\n? \n?\n?\n?\n?\n?\n?\n? \n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?? \n?\n?\n?\n?\n?\n?\n?\n? \n?\n?\n?\n?? \n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n? \n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?? \n?\n?\n?\n?\n?\n??\n?\n?\n?\n?\n?\n? \n?\n?\n?\n?\n?\n?\n?? \n?\n?\n?\n?\n?\n??\n?\n?\n?\n?\n?? \n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?? \n?\n?\n?\n?\n?\n?\n?\n?? \n?\n?\n?\n?\n?\n?? \n?\n?\n?\n? \n?\n?\n?\n?\n?\n?? \n?\n?\n?\n?\n?\n?? \n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n? \n?\n?\n?\n?\n?\n?\n?? \n?\n?\n?\n?\n?\n?\n?\n?\n? \n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?? \n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n? \n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n? \n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?? \n?\n?\n?\n?\n?\n??\n?\n?\n?\n?\n?\n?? \n?\n?\n?\n?\n?\n?\n?\n?? \n?\n? \n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?? \n?\n?\n?\n?\n?\n?\n??\n?\n?\n?\n?\n?\n?\n?\n?-- \n?\n?\n?\n?\n?? \n?\n?\n? \n?\n?\n?\n?\n?? \n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n? \n?\n?\n?\n?\n?\n?\n?\n? \nKekertsuatsiak\n?\n?\n?\n?\n?\n?\n?\n?? \n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n? \n?\n?\n?\n?\n?\n?\n?\n?\n? \n?\n?\n?\n?\n?\n?? \n?\n?\n?\n?\n?\n?\n?\n?? \n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?? \n?\n?\n?\n?\n?15\n?\n?\n?\n?? \n?\n?\n?\n?\n?\n? \n?\n?\n?\n?\n? \n?\n?\n?\n?\n?\n?\n?\n?? \n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?? \n?\n?\n?\n?\n?\n?\n?\n??\n?\n?\n?\n?\n?\n?\n?\n?? \n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?? \n?\n?\n?\n?\n?\n?\n?\n?120\n?\n?\n? \n?\n?40\n?\n?? \n?\n?\n?\n?\n?\n?\n?\n?\n?? \n?\n?\n?\n?\n?? \n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?? \n?\n??\nAs \nan \nartist, \nconnection \nis \nvery \nimportant \nto \nme. \nThrough \nmy \nwork \nI'm \ntrying \nto \narticulate \nthat \nhumans \nare \nnot \nseparate \nfrom \nnature \nand \nthat \neverything \nis \ninterconnected. \nI \nfirst \nwent \nto \nAntarctica \nalmost \n10 \nyears \nago, \nwhere \nI \nsaw \nmy \nfirst \nicebergs. \nI \nwas \nin \nawe. \nMy \nheart \nbeat \nfast, \nmy \nhead \nwas \ndizzy, \ntrying \nto \ncomprehend \nwhat \nit \nwas \nthat \nstood \nin \nfront \nof \nme.",
        "6e4c92b7-bf7c-40d2-b716-b38e5d8065e2": "?? \n?\n?\n?\n?\n?\n?\n?\n?\n?? \n?\n?\n?\n?\n?? \n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?? \n?\n??\nAs \nan \nartist, \nconnection \nis \nvery \nimportant \nto \nme. \nThrough \nmy \nwork \nI'm \ntrying \nto \narticulate \nthat \nhumans \nare \nnot \nseparate \nfrom \nnature \nand \nthat \neverything \nis \ninterconnected. \nI \nfirst \nwent \nto \nAntarctica \nalmost \n10 \nyears \nago, \nwhere \nI \nsaw \nmy \nfirst \nicebergs. \nI \nwas \nin \nawe. \nMy \nheart \nbeat \nfast, \nmy \nhead \nwas \ndizzy, \ntrying \nto \ncomprehend \nwhat \nit \nwas \nthat \nstood \nin \nfront \nof \nme. \nThe \nicebergs \naround \nme \nwere \nalmost \n200 \nfeet \nout \nof \nthe \nwater, \nand \nI \ncould \nonly \nhelp \nbut \nwonder \nthat \nthis \nwas \none \nsnowflake \non \ntop \nof \nanother \nsnowflake, \nyear \nafter \nyear. \nIcebergs \nare \nborn \nwhen \nthey \ncalve \noff \nof \nglaciers \nor \nbreak \noff \nof \nice \nshelves. \nEach \niceberg \nhas \nits \nown \nindividual \npersonality. \nThey \nhave \na \ndistinct \nway \nof \ninteracting \nwith \ntheir \nenvironment \nand \ntheir \nexperiences. \nSome \nrefuse \nto \ngive \nup \nand \nhold \non \nto \nthe \nbitter \nend, \nwhile \nothers \ncan't \ntake \nit \nanymore \nand \ncrumble \nin \na \nfit \nof \ndramatic \npassion. \nIt's \neasy \nto \nthink, \nwhen \nyou \nlook \nat \nan \niceberg, \nthat \nthey're \nisolated, \nthat \nthey're \nseparate \nand \nalone, \nmuch \nlike \nwe \nas \nhumans \nsometimes \nview \nourselves. \nBut \nthe \nreality \nis \nfar \nfrom \nit. \nAs \nan \niceberg \nmelts, \nI \nam \nbreathing \nin \nits \nancient \natmosphere. \nAs \nthe \niceberg \nmelts, \nit \nis \nreleasing \nmineral-rich \nfresh \nwater \nthat \nnourishes \nmany \nforms \nof \nlife. \nI \napproach \nphotographing \nthese \nicebergs \nas \nif \nI'm \nmaking \nportraits \nof \nmy \nancestors, \nknowing \nthat \nin \nthese \nindividual \nmoments \nthey \nexist \nin \nthat \nway \nand \nwill \nnever \nexist \nthat \nway \nagain. \nIt \nis \nnot \na \ndeath \nwhen \nthey \nmelt; \nit \nis \nnot \nan \nend, \nbut \na \ncontinuation \nof \ntheir \npath \nthrough \nthe \ncycle \nof \nlife. \nSome \nof \nthe \nice \nin \nthe \nicebergs \nthat \nI \nphotograph \nis \nvery \nyoung \n-- \na \ncouple \nthousand \nyears \nold. \nAnd \nsome \nof \nthe \nice \nis \nover \n100,000 \nyears \nold. \nThe \nlast \npictures \nI'd \nlike \nto \nshow \nyou \nare \nof \nan \niceberg \nthat \nI \nphotographed \nin \nQeqetarsuaq, \nGreenland. \nIt's \na \nvery \nrare \noccasion \nthat \nyou \nget \nto \nactually \nwitness \nan \niceberg \nrolling. \nSo \nhere \nit \nis. \nYou \ncan \nsee \non \nthe \nleft \nside \na \nsmall \nboat. \nThat's \nabout \na \n15-foot \nboat. \nAnd \nI'd \nlike \nyou \nto \npay \nattention \nto \nthe \nshape \nof \nthe \niceberg \nand \nwhere \nit \nis \nat \nthe \nwaterline. \nYou \ncan \nsee \nhere, \nit \nbegins \nto \nroll, \nand \nthe \nboat \nhas \nmoved \nto \nthe \nother \nside, \nand \nthe \nman \nis \nstanding \nthere.",
        "4d033d08-b2aa-4591-b3aa-7ef013da5e47": "It's \na \nvery \nrare \noccasion \nthat \nyou \nget \nto \nactually \nwitness \nan \niceberg \nrolling. \nSo \nhere \nit \nis. \nYou \ncan \nsee \non \nthe \nleft \nside \na \nsmall \nboat. \nThat's \nabout \na \n15-foot \nboat. \nAnd \nI'd \nlike \nyou \nto \npay \nattention \nto \nthe \nshape \nof \nthe \niceberg \nand \nwhere \nit \nis \nat \nthe \nwaterline. \nYou \ncan \nsee \nhere, \nit \nbegins \nto \nroll, \nand \nthe \nboat \nhas \nmoved \nto \nthe \nother \nside, \nand \nthe \nman \nis \nstanding \nthere. \nThis \nis \nan \naverage-size \nGreenlandic \niceberg. \nIt's \nabout \n120 \nfeet \nabove \nthe \nwater, \nor \n40 \nmeters. \nAnd \nthis \nvideo \nis \nreal \ntime. \nAnd \njust \nlike \nthat, \nthe \niceberg \nshows \nyou \na \ndifferent \nside \nof \nits \npersonality. \nThank \nyou.\nAs \nan \nartist, \nconnection \nis \nvery \nimportant \nto \nme. \nAnd \nthrough \nmy \nartwork, \nI \ntry \nto \nillustrate \nthat \nhumans \nare \nnot \nseparate \nfrom \nnature, \nbut \nthat \neverything \nis \nconnected \nto \neach \nother. \nI \nwent \nto \nAntarctica \nfor \nthe \nfirst \ntime \nabout \n10 \nyears \nago, \nand \nI \nsaw \nthe \niceberg \nfor \nthe \nfirst \ntime. \nI \nfelt \nawe. \nAnd \nmy \nheart \nwas \npounding, \nmy \nhead \nwas \npounding, \ntrying \nto \nfigure \nout \nwhat \nwas \ngoing \non \nin \nfront \nof \nme. \nAnd \nthe \niceberg \nthat \nwas \nright \nnext \nto \nme \nwas \nfloating \nalmost \n200 \nfeet \nabove \nthe \nsurface \nof \nthe \nwater, \nand \nI \njust \ncouldn't \nhelp \nbut \nfeel \nthat \nthis \nwas \na \nsnowflake \ncovering \nanother \nsnowflake, \nforming \nover \nand \nover \nand \nover \nagain. \nThe \nformation \nof \nglaciers \nis \nwhen \nthey \nbreak \noff \nfrom \nglaciers, \nor \nbreak \noff \nfrom \nice \nshelves. \nEvery \niceberg \nhas \nits \nown \nunique \npersonality. \nThey \ninteract \nin \na \nvery \ndistinctive \nway \nwith \nthe \nenvironment \naround \nthem \nand \nwith \nthe \ncircumstances \naround \nthem. \nSome \nglaciers \nrefused \nto \ncompromise \nand \ninsisted, \nand \nsome \nglaciers \ncouldn't \nstand \nthe \nheat \nof \npassion \nas \nit \npoured \ndown \nthe \nice. \nWhen \nyou \nlook \nat \nicebergs, \nit's \neasy \nto \nthink \nthat \nthey're \nisolated, \nthat \nthey're \nindependent, \nthat \nthey're \nseparate, \nthat \nthey're \nmore \nlike \nthe \nway \nwe \nthink \nabout \nourselves \nsometimes. \nBut \nthe \nreality \nis \nmuch \nmore \nthan \nthat. \nAnd \nas \nthe \nglacier \nmelts, \nI \nbreathe \nin \nits \nancient \nsmell. \nAnd \nas \nthe \nglacier \nmelts, \nit \nreleases \nfresh \nwater \nof \nminerals \nthat \nnourish \neverything. \nI \nstarted \nphotographing \nthese \nicebergs \nlike \nI \nwas \nphotographing \nmy \nancestors, \nand \nI \nlearned \nthat \nin \nthese \nindividual \nmoments, \nthe \nicebergs \nexisted \nin \nthat \nway, \nbut \nthey \nnever \nexisted \nlike \nthat \nagain. \nWhen \nthey \nmelt, \nit's \nnot \nabout \ndeath; \nit's \nnot \nabout \nthe \nend, \nit's \nabout \nthe \ncontinuation \nof \na \nlife-long \npath.",
        "ea33af40-1308-4eb9-8ea3-5983ffd84ccf": "And \nas \nthe \nglacier \nmelts, \nI \nbreathe \nin \nits \nancient \nsmell. \nAnd \nas \nthe \nglacier \nmelts, \nit \nreleases \nfresh \nwater \nof \nminerals \nthat \nnourish \neverything. \nI \nstarted \nphotographing \nthese \nicebergs \nlike \nI \nwas \nphotographing \nmy \nancestors, \nand \nI \nlearned \nthat \nin \nthese \nindividual \nmoments, \nthe \nicebergs \nexisted \nin \nthat \nway, \nbut \nthey \nnever \nexisted \nlike \nthat \nagain. \nWhen \nthey \nmelt, \nit's \nnot \nabout \ndeath; \nit's \nnot \nabout \nthe \nend, \nit's \nabout \nthe \ncontinuation \nof \na \nlife-long \npath. \nI \nphotographed \nglaciers, \nand \nsome \nof \nthem \nwere \nvery \nyoung \n-- \nthousands \nof \nyears \nold. \nSome \nof \nthe \nice \nhas \nbeen \nthere \nfor \nmore \nthan \n100,000 \nyears. \nAnd \nthe \nlast \npicture \nI \nwant \nto \nshow \nyou \nis \nan \niceberg \nthat \nI \nphotographed \nin \nKekertsuatsiak \non \nthe \nisland \nof \nGreenland. \nIt's \na \nvery \ndifficult \nopportunity \nto \nactually \nwitness \nthe \nrolling \nof \nan \niceberg. \nSo \nthis \nis \nwhat \nit \nlooks \nlike. \nYou \ncan \nsee \na \nlittle \nboat \non \nthe \nleft. \nThis \nis \na \n15-foot \nboat. \nI \nwant \nyou \nto \nnotice \nthat \nthe \nshape \nof \nthe \niceberg \nchanges \nas \nit \nmoves \nover \nthe \nsurface. \nAnd \nhere \nyou \nsee \nit \nrolling, \nand \nthe \nboat \nmoves \nto \nthe \nother \nside, \nand \na \nman \nis \nstanding \nthere. \nThis \nis \nan \naverage \nsize \nglacier \nin \nGreenland. \nIt \nfloats \nabout \n120 \nfeet \nup \nor \n40 \nmeters \nabove \nthe \nsurface. \nThis \nvideo \nwas \ntaken \nin \nreal \ntime. \nAnd \nlike \nthis \niceberg, \nthey \nshow \nyou \ndifferent \naspects \nof \ntheir \npersonality. \nThank \nyou.\nAnd \nas \nan \nartist, \nconnection \nis \nvery \nimportant \nto \nme. \nThrough \nmy \nartwork, \nI \ntry \nto \nconvey \nthe \nidea \nthat \nhumans \nare \nnot \nseparated \nfrom \nnature, \nbut \nthat \neverything \nis \nconnected \nto \neach \nother. \nWhen \nI \nfirst \nwent \nto \nAntarctica \nabout \n10 \nyears \nago, \nI \nsaw \nfor \nthe \nfirst \ntime \nicebergs. \nAnd\n \nI \nfelt \nawe. \nMy \nheart \nwas \nshaking, \nmy \nhead \nwas \nshaking, \ntrying \nto \nunderstand \nwhat \nwas \nin \nfront \nof \nme. \nThe \nicebergs \naround \nme \nwere \nfloating \nalmost \n200 \nfeet \nabove \nthe \nsurface \nof \nthe \nwater, \nand \nI \ncould \nonly \nfeel \nhow \nstrange \nit \nwas \nthat \nthis \nwas \na \nsnowflake \ncovering \nanother \nsnowflake, \nforming \nover \nand \nover \nagain \nover \nand \nover \nagain. \nAnd\n \nicebergs \nform \nwhen \nthey \nbreak \noff \nfrom \nglaciers \nor \nwhen \nthey \nbreak \noff \nfrom \nice \nshelves.\n \nAnd\n \neach \niceberg \nhas \nits \nown \nunique \npersonality. \nThey \ninteract \nin \na \nvery \ndistinctive \nway \nwith \nthe \nenvironment \naround \nthem \nand \nwith \nthe \ncircumstances \nin \nwhich \nthey're \nlocated. \nSome \nicebergs \nrefuse \nto \nsettle \ndown, \nand \nsome \nicebergs \ncan't \nstand \nthe \nheat \nof \npassion \nthat \npours \ndown \nand \nbreaks \nice.",
        "9eb6b1c7-40c2-463b-ae00-61432d36be0a": "And\n \nicebergs \nform \nwhen \nthey \nbreak \noff \nfrom \nglaciers \nor \nwhen \nthey \nbreak \noff \nfrom \nice \nshelves.\n \nAnd\n \neach \niceberg \nhas \nits \nown \nunique \npersonality. \nThey \ninteract \nin \na \nvery \ndistinctive \nway \nwith \nthe \nenvironment \naround \nthem \nand \nwith \nthe \ncircumstances \nin \nwhich \nthey're \nlocated. \nSome \nicebergs \nrefuse \nto \nsettle \ndown, \nand \nsome \nicebergs \ncan't \nstand \nthe \nheat \nof \npassion \nthat \npours \ndown \nand \nbreaks \nice. \nAnd\n \nwhen \nyou \nlook \nat \nicebergs, \nit's \neasy \nto \nthink \nthat \nthey're \nisolated, \nthat \nthey're \nindependent, \nthat \nthey're \nindividual, \nthat \nthey're \nmore \nlike \nthe \nway \nwe \nthink \nabout \nourselves \nsometimes. \nBut \nthe \nreality \nis \nmuch \nmore \nthan \nthat. \nAs \nthe \nicebergs \nmelt, \nI \nbreathe \nin \nthe \nsmell \nof \nits \nancient \npast. \nAs \nthe \nicebergs \nmelt, \nthey \nrelease \nfresh \nwater \nthat \nis \nrich \nin \nminerals \nthat \nfeed \neverything. \nAnd\n \nI'm \nphotographing \nthese \nicebergs \nlike \nI'm \nphotographing \nmy \nancestors, \nand \nI'm \nlearning \nthat \nin \nthese \nindividual \nmoments, \nicebergs \nused \nto \nexist \nin \nthat \nway \nand \nwill \nnever \nbe \nthe \nsame \nagain. \nWhen \nthey \nmelt, \nit's \nnot \nabout \ndeath; \nit's \nnot \nabout \nthe \nend, \nbut \nit's \nabout \na \ncontinuation \nof \na \nlifetime. \nAnd\n \nthe \nicebergs \nI've \nphotographed, \nsome \nof \nthem \nare \nvery \nyoung \n-- \nthousands \nof \nyears \nold. \nAnd\n \nsome \nof \nthem \nare \nmore \nthan \n100,000 \nyears \nold. \nAnd\n \nthe \nlast \npicture \nI \nwant \nto \nshow \nyou \nis \na \niceberg \nthat \nI \nphotographed \non \nKekertsuatsiak \nin \nGreenland.\n \nAnd\n \nit's \na \nvery \ndifficult \nopportunity \nfor \nyou \nto \nactually \nwitness \nthe \nrolling \nof \na \niceberg. \nSo \nhere \nit \nis. \nOn \nthe \nleft \nyou \ncan \nsee \na \nlittle \nboat. \nIt's \na \nlittle \nboat \nabout \n15 \nfeet \nlong. \nAnd\n \nI \nwant \nyou \nto \nnotice \nthat \nthe \nshape \nof \nthe \niceberg \nchanges \nas \nit \nfloats \nover \nthe \nsurface \nof \nthe \nwater. \nAnd\n \nhere \nyou \nsee \nit \nstart \nto \nroll, \nand \nthe \nboat \nmoves \nto \nthe \nother \nside, \nand \na \nman \nis \nstanding \nthere. \nAnd\n \nthis \nis \nan \naverage \nsize \nIcelandic \niceberg. \nAnd\n \nit \nfloats \nabout \n120 \nfeet \nabove \nthe \nsurface \nof \nthe \nwater, \nor \n40 \nmeters. \nAnd\n \nthis \nvideo \nwas \ntaken \nin \nreal \ntime. \nAnd\n \nlike \nthese \nicebergs, \nthey \nshow \nyou \ndifferent \naspects \nof \ntheir \npersonality. \nThank \nyou.\nSOURCE\nTARGET\nmBART25 \nSENT-MT\nmBART25 \nDOC-MT\nAs \nan \nartist, \nconnection \nis \nvery \nimportant \nfor \nme. \nThrough \nmy \nartwork, \nI \ntry \nto \nillustrate \nthat \nhuman \nbeings \nare \nnot \nseparated \nfrom \nnature, \nbut \neach \none \nof \nthem \nis \ninterconnected. \nAbout \n10 \nyears \nago, \nI \nfirst \nwent \nto \nAntarctica, \nand \nI \nsaw \nmountains \nfor \nthe \nfirst \ntime. \nI \nfelt \nawe.",
        "87ab3b1d-e3e8-4a94-8866-c77e60362507": "And\n \nthis \nvideo \nwas \ntaken \nin \nreal \ntime. \nAnd\n \nlike \nthese \nicebergs, \nthey \nshow \nyou \ndifferent \naspects \nof \ntheir \npersonality. \nThank \nyou.\nSOURCE\nTARGET\nmBART25 \nSENT-MT\nmBART25 \nDOC-MT\nAs \nan \nartist, \nconnection \nis \nvery \nimportant \nfor \nme. \nThrough \nmy \nartwork, \nI \ntry \nto \nillustrate \nthat \nhuman \nbeings \nare \nnot \nseparated \nfrom \nnature, \nbut \neach \none \nof \nthem \nis \ninterconnected. \nAbout \n10 \nyears \nago, \nI \nfirst \nwent \nto \nAntarctica, \nand \nI \nsaw \nmountains \nfor \nthe \nfirst \ntime. \nI \nfelt \nawe. \nMy \nheart \nwas \nrapidly \nwiped \nout, \ntrying \nto \nfigure \nout \nwhat \nwas \ngoing \non \nin \nfront \nof \nme. \nThe \niceberg \nthat \nfloats \naround \nme \nalmost \n200 \nfeet \nof \nwater, \nand \nI \ncan \nonly \nfeel \nstrange \nthat \nthis \nsnow \ncovers \nanother \npiece \nof \nsnow \nfor \na \nyear \nand \nyears. \nThe \nform \nof \nthe \niceberg \nis \nthat \nwhen \nthey \nbreak \napart \nfrom \nthe \nglacier \nor \nbreaking \noff \nthe \nshelves \nof \nice. \nEvery \niceberg \nhas \ntheir \nown \npersonality. \nThey're \ninteracting \nwith \ntheir \nsurrounding \nenvironment \nin \na \nvery \ndifferent \nway. \nSome \nof \nthe \nice \nmountains \nrefused \nto \ncompromise, \nand \nsome \nother \nmountains \nof \nice \ncan't \nendure, \nand \nthe \nwater \ncollapses \nduring \na \nviolent \nice. \nAnd \nwhen \nyou \nlook \nat \nthe \niceberg, \nit's \neasy \nto \nthink \nthat \nthey're \nall \nisolated, \nand \nthey're \nindependent, \nthey're \nunited \nalone, \nand \nsometimes \nwe \nthink \nabout \nourselves. \nBut \nit's \nmore \nthan \nthat. \nAs \nthe \nice \nmelts, \nI \nbreathe \nit, \nancient \nsmell. \nAnd \nas \nthe \nice \nmelts, \nit \nreleases \nthe \nrich \nminerals \nand \nit \nfeeds \n20,000. \nI'm \nphotographing \nthese \nmountains \nof \nice, \nand \nI'm \nphotographing \nmy \nancestors, \nand \nI'm \ntaking \npictures \nof \nmy \nancestors, \nand \nI've \nlearned \nthat \nin \nthese \nindividual \nmoments \nof \nice \nis \nthere \nin \na \nway \nthat's \nthere, \nbut \nit's \nnever \ngoing \nto \nexist \nagain. \nWhen \nthey \nmelt, \nit's \nnot \ndead; \nit's \nnot \nthe \nend; \nit's \nthe \nend, \nit's \na \ncontinuation \nof \ncontinuity \nto \nlife. \nThe \niceberg \nthat \nI \nphotographed, \nsome \nof \nthe \nice \nis \nvery \nyoung \n-- \nthe \nage \nof \nthousands \nof \nyears. \nSome \nof \nthem \nare \nice \nover \n100,000 \nyears. \nAnd \nthe \nlast \npicture \nI \nwant \nto \nshow \nyou \nis \none \nof \nthe \nices \nI \nmade \nin \nGreenland. \nIt's \na \nvery \ndifficult \nopportunity, \nand \nyou \ncan \nactually \nsee \na \nhill \nrolling. \nSo \nthis \nis \nwhat \nit \nlooks \nlike. \nAnd \non \nthe \nleft \nyou \ncan \nsee \na \nlittle \nboat. \nThis \nis \na \nship \nabout \n15 \nfeet. \nAnd \nI \nwant \nyou \nto \nnotice \nthat \nthe \nshape \nof \nthe \niceberg \nis \nshaped \nin \nthe \nwater.",
        "3b2895af-baa1-4679-ba42-7f2411a5a1a1": "Some \nof \nthem \nare \nice \nover \n100,000 \nyears. \nAnd \nthe \nlast \npicture \nI \nwant \nto \nshow \nyou \nis \none \nof \nthe \nices \nI \nmade \nin \nGreenland. \nIt's \na \nvery \ndifficult \nopportunity, \nand \nyou \ncan \nactually \nsee \na \nhill \nrolling. \nSo \nthis \nis \nwhat \nit \nlooks \nlike. \nAnd \non \nthe \nleft \nyou \ncan \nsee \na \nlittle \nboat. \nThis \nis \na \nship \nabout \n15 \nfeet. \nAnd \nI \nwant \nyou \nto \nnotice \nthat \nthe \nshape \nof \nthe \niceberg \nis \nshaped \nin \nthe \nwater. \nAnd \nhere \nyou \nsee \nit \nstarts \nrolling, \nlittle \nboats \nmoving \nto \nthe \nother \nside, \nand \na \nman \nis \nstanding \nthere. \nThis \nis \nan \naverage \niceberg \nin \nGreenland. \nIt's \nabout \n120 \nfeet \ntall, \nor \n40 \nmeters. \nThis \nis \na \nreal \ntime \nlapse. \nLike \nthis \niceberg, \nthey \nshow \nyou \ndifferent \naspects \nof \ntheir \npersonality. \nThank \nyou.\nAs \nan \nartist, \nas \nan \nartist, \nas \nan \nartist, \nas \nan \nartist, \nas \nan \nartist, \nas \nan \nartist, \nas \nan \nartist, \nas \nan \nartist, \nas \nan \nartist, \nas \nan \nartist. \nI'm \ngoing \nto \nfocus \non \nthe \nglacier \nand \nthe \nglacier \nand \nthe \nglacier \nand \nthe \nglacier. \nThere's \na \nlot \nof \nice \nin \nthe \nice \nin \nthe \nice, \nand \nthere's \na \nlot \nof \nice \nin \nthe \nice, \nand \nthere's \na \nlot \nof \nice \nin \nthe \nice, \nand \nthere's \na \nlot \nof \nice \nin \nthe \nice \nin \nthe \nice. \nIt's \na \nice \nthat's \nice \nthat's \nmelted \nfrom \nthe \nice \nof \nthe \nice \nthat's \nmelted \nfrom \nthe \nice \nof \nthe \nice \nthat's \nmelted \nfrom \nthe \nice \nof \nthe \nice \nthat's \nmelted \nfrom \nthe \nice \nof \nthe \nice \nthat \nI've \nlost. \nThere's \na \nlot \nof \nice \nthat \nI'm \ngoing \nto \nshow \nyou \nsome \npictures \nthat \nI'm \ngoing \nto \nshow \nyou. \nAnd \nyou \ncan \nsee \nthat \nit's \nmoving \nto \nthe \ntop \nof \nit, \nand \nit's \nmoving \nto \nthe \ntop \nof \nit.\nRandom \nSENT-MT\nRandom \nDOC-MTFigure 6: An Example of Document-level translation from mBART25 Sent-MT and Doc-MT , held out from\nthe test set of TED15 Zh-En. The Doc-MT system produces much \ufb02uent and coherent translation which is closer\nto the reference translation. For instance, Doc-MT model produces several \u201cAnd \u201d to connect sentences to make it\nreads better, while the Sent-MT model does not contain global knowledge and produce sentences independently.\nBesides, both systems produce much better translations than models without pre-training where the non-pretrained\nDoc-MT model completely fails to produce readable translation output.",
        "7de78b96-4a74-481e-ba0e-d7fdf7f5bced": "?\n?\n?\n?\n?\n?\n?\n?\n?,\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n??\nIn \nresponse \nto \nthe \ngovernment's \nsilence, \nthe \nChief \nMedical \nOfficers' \nCouncil \nis \ncalling \ntoday \nfor \na \nspecial \nsession \nat \nthe \nCouncil \nof \nthe \nBritish \nMedical \nAssociation\n, \nwhich \nis \na \nlong-term \ninitiative \nto \nupgrade \nlabor \nfrom \nSeptember.\nIn \nresponse \nto \nthe \ngovernment's \nsilence, \nJDC \nexec \nhas \ntoday \nmade \na \nformal \nrequest \nfor \na \nspecial \nmeeting \nof \nBMA \nCouncil\n \nto \nauthorise \na \nrolling \nprogramme \nof \nescalated \nindustrial \naction\n \nbeginning \nin \nearly \nSeptember.\nIn \nresponse \nto \nthe \ngovernment's \nsilence, \nthe \nCouncil \nof \nChief \nMedical \nOfficers\n \nhas \nformally \nrequested \ntoday \nthe \nRoyal \nCollege \nof \nPhysicians\n \nto \nhold \na \nspecial \nmeeting \nto \napprove \na \nlong-term\n \nworkforce \naction\n \nthat \nstarts \nin \nSeptember.\nIn \nresponse \nto \nthe \ngovernment's \nsilence, \nthe \nBoard \nof \nPrimary \nDoctors\n \nhas \ntoday \nformally \nasked \nthe \nBritish \nMedical \nAssociation\n \nto \nhold \na \nspecial \nmeeting \nto \napprove \na \nlong-term \nplan \nthat \nstarts \nin \nthe \nbeginning \nof \nSeptember.\nSOURCE\nZh\nTARGET\nEn\nmBART25 \nJa-En\nmBART25 \nKo-En\nmBART25 \nZh-En\n?\n?\n?\n?\n?\n?\n?\n? \n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n? \n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n? \n?\n?\n?\n??\n? \n?\n?\n?\n?\n??\n?\n?\n?\n?\n?\n? \n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n? \n?\n?\n?\n?\n?\nIt's \ncommonplace \nin \ncountries \nlike \nCanada \nand \nthe \nUnited \nStates \nand \nmany \nother\n?\n? \ncountries\n, \nbut \nit's \nnot \ncommonplace \nin \npoor \ncountries, \nin \npatriarchal \nsocieties, \nin \nclan \nsocieties\n, \nwhere \nschooling\n \nis \na \nbig \ndeal \nfor \ngirls.\nIt \nmay \nbe \ntaken \nfor \ngranted \nin \nCanada, \nin \nAmerica, \nin \nmany \ndeveloped \ncountries, \nbut \nin \npoor \ncountries, \nin \npatriarchal \nsocieties, \nin \ntribal \nsocieties, \nit's \na \nbig \nevent \nfor \nthe \nlife \nof \ngirl.\nIn \nCanada, \nin \nthe \nUnited \nStates, \nand \nmany \nother \ndeveloped \ncountries, \nit's \ntaken \nfor \ngranted \nthat \nin \npoor \ncountries, \nin \npatriarchal \nsocieties, \nin\n \ntribal \nsocieties\n, \neducation\n \nis \nvery \nimportant \nfor \ngirls.\nIt's \ncommonplace \nin \nCanada, \nin \nthe \nU.S., \nand \nin \nmany \nother \ncountries \nin \nthe \nworld, \nin \npoor \ncountries, \nin \npatriarchal \nsocieties, \nin \nethnic \nsocieties, \nthat \neducation\n \nis \na \npriority \nfor \ngirls.\nSOURCE\nJa\nTARGET\nEn\nmBART25 \nJa-En\nmBART25 \nKo-En\nmBART25 \nZh-En\n??? \n???, \n? \n?? \n???? \n?? \n?? \n??? \n??? \n?? \n?????. \n??? \n? \n???? \n? \n? \n??? \n??? \n??? \n?? \n1,000 \n?? \n?? \n??? \n??? \n??? \n??? \n??? \n???? \n????.",
        "bf9b0fa8-9388-4f5e-9675-c3a8259d7f07": "SOURCE\nJa\nTARGET\nEn\nmBART25 \nJa-En\nmBART25 \nKo-En\nmBART25 \nZh-En\n??? \n???, \n? \n?? \n???? \n?? \n?? \n??? \n??? \n?? \n?????. \n??? \n? \n???? \n? \n? \n??? \n??? \n??? \n?? \n1,000 \n?? \n?? \n??? \n??? \n??? \n??? \n??? \n???? \n????.\nThe \nconventional \nwisdom \nis \nthat \nthe \nonly \nway \nto \nget \nhigher \nresolution \nis \nwith \nbigger \nmagnets\n, \nbut \nat \nthis \npoint, \nbigger \nmagnets \nonly \ngive \nus \na \nlittle \nbit \nof \nresolution \nimprovement, \nnot \n1,000 \ntimes \nas \nmuch \nas \nwe \nneed.\nConventional \nwisdom \nsays \nthe \nonly \nway \nto \nget \nhigher \nresolution \nis \nwith \nbigger \nmagnets, \nbut \nat \nthis \npoint \nbigger \nmagnets \nonly \noffer \nincremental \nresolution \nimprovements, \nnot \nthe \nthousandfold \nwe \nneed\nIn \nthe \nconventional \nwisdom, \nthe \nonly \nway \nto \nget \nhigher \nresolution \nis \nwith \nthe \nbig \ncushions\n, \nbut \nat \nthis \npoint, \nthe \nbigger \ncushions \ngive \nus \njust \na \nlittle \nbit \nmore \nresolution \nthan \nwe \nneed \nto \nget \n1,000 \ntimes \nbetter.\nAnd \nthe \nconventional \nwisdom \nis \nthat \nthe \nonly \nway \nto \nget \na \nhigher \nresolution \nis \nwith \nlarge \njellyfish\n, \nbut \nat \nthis \npoint \nin \ntime, \nbig \njellyfish \nis \nonly \ngoing \nto \nprovide \nus \nwith \n1,000 \ntimes \nas \nmuch \nresolution \nas \nwe \nneed \nwith \njust \na \nlittle \nbit \nof \nresolution \nimprovement.\nSOURCE\nKo\nTARGET\nEn\nmBART25 \nJa-En\nmBART25 \nKo-En\nmBART25 \nZh-EnFigure 7: Examples of Unsupervised MT via Language Transfer between Ja,Ko,Zh\u2192En. We mark the su-\npervised settings in red. All three languages have quite different character sets (Ja and Zh shares part of the Chinese\ncharacters) and syntactic structures. However, they are still culturally and historically correlated, which we assume\ncan be captured through pre-training. For all cases, if we \ufb01ne-tune the mBART25 model on any pair, the resulted\nmodel directly translates well in the other two pairs without seeing any corresponded parallel sentences. We also\nsee failure cases. For instance (the 3rd example), only the supervised model translates \u201c \uc790\uc11d\u201d into \u201cmagents\u201d cor-\nrectly, while the Ja-En and Zh-En guess with irreverent words \u201ccushions\u201d and \u201cjelly\ufb01sh\u201d, respectively. Also, in the\n2nd example, the Ko-En model fails to translate \u201cdeveloped\u201d and copies the source tokens. We suspect it is because\nthe pre-training stage biases the output distribution."
    },
    "relevant_docs": {
        "9336b433-75b7-4b7b-a7ca-f838009455a6": [
            "f918a968-3163-4413-87d4-e2fbee0e7e65"
        ],
        "d72faa12-5169-4152-b236-6ca297bc59cd": [
            "f918a968-3163-4413-87d4-e2fbee0e7e65"
        ],
        "a721cbcf-84b2-4243-b07e-2a50c12ca70e": [
            "f918a968-3163-4413-87d4-e2fbee0e7e65"
        ],
        "a4734c60-f03a-4b67-a771-eb1cd84a6f4e": [
            "f918a968-3163-4413-87d4-e2fbee0e7e65"
        ],
        "98a1132e-aee9-4b2d-a805-03583d13c641": [
            "f918a968-3163-4413-87d4-e2fbee0e7e65"
        ],
        "0a360840-a02a-4212-a680-998d3683cd0e": [
            "a20eca33-0755-40bf-bdd0-290713100348"
        ],
        "24420e01-41d0-4be9-84b1-89e45d9d3132": [
            "a20eca33-0755-40bf-bdd0-290713100348"
        ],
        "4db872bc-c7f3-41b9-aafd-2fa50e373333": [
            "a20eca33-0755-40bf-bdd0-290713100348"
        ],
        "38f459b3-e5dd-49fd-81a0-7087d3d72d64": [
            "a20eca33-0755-40bf-bdd0-290713100348"
        ],
        "cbd986c8-2773-44dd-9648-dc0fa12d025f": [
            "a20eca33-0755-40bf-bdd0-290713100348"
        ],
        "041e36e5-4c77-4a5b-b072-8b43fbc5514a": [
            "7ccd09b5-582d-4e4a-8da8-0eb6d5d1f823"
        ],
        "fb98d22a-5ada-4fa6-9df4-66d752a0f5bd": [
            "7ccd09b5-582d-4e4a-8da8-0eb6d5d1f823"
        ],
        "c6a0e42d-ad35-40f0-b562-db6ccb8944fc": [
            "7ccd09b5-582d-4e4a-8da8-0eb6d5d1f823"
        ],
        "cd53688c-cd00-4dae-bd18-3e6df2e4ad61": [
            "7ccd09b5-582d-4e4a-8da8-0eb6d5d1f823"
        ],
        "2265ccd8-924e-448f-9c6e-2bf6ddc38179": [
            "ab7f18d4-2e6b-4b9c-bfe6-cfc4dc2ef621"
        ],
        "7a5e18ef-0aef-402a-aa93-ac29186ed15c": [
            "ab7f18d4-2e6b-4b9c-bfe6-cfc4dc2ef621"
        ],
        "77124637-58fa-4562-8541-7a77fe09206b": [
            "ab7f18d4-2e6b-4b9c-bfe6-cfc4dc2ef621"
        ],
        "7dabec49-ac3a-4e3e-b023-38e66ec1e436": [
            "ab7f18d4-2e6b-4b9c-bfe6-cfc4dc2ef621"
        ],
        "c6ecca4c-19d1-41cb-b816-066702dece2a": [
            "ab7f18d4-2e6b-4b9c-bfe6-cfc4dc2ef621"
        ],
        "7fae64ba-b940-420f-9fb7-9a0d2242d1c7": [
            "ab7f18d4-2e6b-4b9c-bfe6-cfc4dc2ef621"
        ],
        "ae9feef4-2a08-4dee-9fea-419136e40d56": [
            "ab7f18d4-2e6b-4b9c-bfe6-cfc4dc2ef621"
        ],
        "1e3d142e-32e2-46c4-a857-87cd03898e4f": [
            "ab7f18d4-2e6b-4b9c-bfe6-cfc4dc2ef621"
        ],
        "dc5005ae-0a7e-4497-9341-2e6d3e9aeb39": [
            "ab7f18d4-2e6b-4b9c-bfe6-cfc4dc2ef621"
        ],
        "c30b9fe6-750e-4fdc-baf4-54fd59eb271e": [
            "ab7f18d4-2e6b-4b9c-bfe6-cfc4dc2ef621"
        ],
        "6d4567ed-0249-4352-995b-1beb8ce0cd6b": [
            "52b995ea-52d1-4771-a99b-5f22725c1720"
        ],
        "0f945de9-813e-4106-8d31-89d88319c952": [
            "52b995ea-52d1-4771-a99b-5f22725c1720"
        ],
        "9cbace1b-48bd-4142-ac42-b003f4ba2795": [
            "52b995ea-52d1-4771-a99b-5f22725c1720"
        ],
        "eea4896a-1ef6-4aba-962b-22e12721df87": [
            "52b995ea-52d1-4771-a99b-5f22725c1720"
        ],
        "7f383261-7e13-4e5f-a2fc-75291ca67758": [
            "52b995ea-52d1-4771-a99b-5f22725c1720"
        ],
        "323d8927-81db-444b-9535-8f3637917675": [
            "9e3dcb5a-58cb-4bbe-af09-fce0e96bda9f"
        ],
        "f4b418ed-a6f0-4bc4-8835-234ad9c0755f": [
            "9e3dcb5a-58cb-4bbe-af09-fce0e96bda9f"
        ],
        "b6b0e8f1-929e-439e-92d6-1aa78fe6029c": [
            "9e3dcb5a-58cb-4bbe-af09-fce0e96bda9f"
        ],
        "8c6eea71-827f-4ae5-9272-5325d186a5a1": [
            "9e3dcb5a-58cb-4bbe-af09-fce0e96bda9f"
        ],
        "9ff3d80c-991f-49ec-862e-14015af1a1c0": [
            "9e3dcb5a-58cb-4bbe-af09-fce0e96bda9f"
        ],
        "25e30c6b-6f33-4c91-b2bd-524425653842": [
            "5dde4e00-4f9c-4428-a279-33a1b97fbffd"
        ],
        "7f6a7de2-02bd-48bc-96fe-e9c9b49fae78": [
            "5dde4e00-4f9c-4428-a279-33a1b97fbffd"
        ],
        "2945e023-781f-4342-9f2a-ed4777b8b384": [
            "5dde4e00-4f9c-4428-a279-33a1b97fbffd"
        ],
        "8cf19e57-1f22-4668-8528-b330bfda48d5": [
            "5dde4e00-4f9c-4428-a279-33a1b97fbffd"
        ],
        "e0e624a4-4692-4e52-9171-bae70107a0a9": [
            "5dde4e00-4f9c-4428-a279-33a1b97fbffd"
        ],
        "7d45a0b7-d272-4780-94da-6a183cb752ff": [
            "b3b5059f-f2fa-48c2-8422-67746f294be0"
        ],
        "c12b6605-69c8-42a5-a358-0da53d5fd450": [
            "b3b5059f-f2fa-48c2-8422-67746f294be0"
        ],
        "a450bcee-5269-4074-9f22-0f2ec9478932": [
            "b3b5059f-f2fa-48c2-8422-67746f294be0"
        ],
        "3de49368-4742-4d64-9d06-e52e8086a930": [
            "b3b5059f-f2fa-48c2-8422-67746f294be0"
        ],
        "1b99e053-5a71-42a4-a3d8-1ff75f006bd1": [
            "b3b5059f-f2fa-48c2-8422-67746f294be0"
        ],
        "b4e46503-4165-4675-877e-c29b0ef2627a": [
            "b3b5059f-f2fa-48c2-8422-67746f294be0"
        ],
        "59faca2c-5dab-43c5-a772-e1f4c553eeb2": [
            "7048f69f-cc2a-41b4-876d-c2a5e40491c0"
        ],
        "3572f92e-fc8b-422b-a5d1-cb7d9cdc4fb8": [
            "7048f69f-cc2a-41b4-876d-c2a5e40491c0"
        ],
        "17524e9d-f6ab-4e87-93c3-642efadd8e38": [
            "7048f69f-cc2a-41b4-876d-c2a5e40491c0"
        ],
        "3b720eb3-d2bc-40eb-87c7-ee86a75037df": [
            "971f1cdb-1173-45c5-93de-490cf6b6b08d"
        ],
        "45cad20b-78da-4d9c-bddc-cf6860a9bc9e": [
            "971f1cdb-1173-45c5-93de-490cf6b6b08d"
        ],
        "cfc12f43-58f2-453c-8a3a-64abc896fef8": [
            "971f1cdb-1173-45c5-93de-490cf6b6b08d"
        ],
        "dda48c91-d4ab-4954-9dc0-9039a6a2de73": [
            "7f0086c3-a7f9-47c9-8e65-031a3c891042"
        ],
        "8373eeea-cbe0-4dc8-a2a7-7a9f6553b691": [
            "7f0086c3-a7f9-47c9-8e65-031a3c891042"
        ],
        "510d4cc2-30c5-4f3b-9c19-f8402365f13d": [
            "7f0086c3-a7f9-47c9-8e65-031a3c891042"
        ],
        "52fc3d0e-06df-49b4-96d8-d0c7d97db9c5": [
            "7f0086c3-a7f9-47c9-8e65-031a3c891042"
        ],
        "26d3a801-427a-462e-af96-2bab0f76205d": [
            "7f0086c3-a7f9-47c9-8e65-031a3c891042"
        ],
        "810bb79b-a473-41ef-94cc-7c0700408b0d": [
            "633b793c-11b2-442f-9bf2-93d036e46b37"
        ],
        "761cf9e5-72c2-4e67-8d7e-b2ba0d745403": [
            "633b793c-11b2-442f-9bf2-93d036e46b37"
        ],
        "05df18af-fe07-4c9b-acb6-11e21309d93b": [
            "633b793c-11b2-442f-9bf2-93d036e46b37"
        ],
        "d4e7b25a-2b5d-47fb-aaa5-f69bd4138da4": [
            "633b793c-11b2-442f-9bf2-93d036e46b37"
        ],
        "68ed8737-49d3-491c-b683-a31dda733fba": [
            "633b793c-11b2-442f-9bf2-93d036e46b37"
        ],
        "28b2e5c5-7e7c-49d8-9ff1-8148a536d25f": [
            "08462388-a0e0-405e-a75b-e4bc6b566a8b"
        ],
        "59dc9b0f-0fb0-433f-a464-32c580255545": [
            "08462388-a0e0-405e-a75b-e4bc6b566a8b"
        ],
        "a526d108-2ce6-46ac-b74c-6a12a0c9cf80": [
            "08462388-a0e0-405e-a75b-e4bc6b566a8b"
        ],
        "a604537c-da99-469c-96d4-a72877a0f21b": [
            "08462388-a0e0-405e-a75b-e4bc6b566a8b"
        ],
        "938abb27-a097-4e9d-9e2b-535227a1a1b6": [
            "08462388-a0e0-405e-a75b-e4bc6b566a8b"
        ],
        "beb095d3-6616-42a8-b89f-fc28058a5793": [
            "48e53904-343b-4173-9dda-beadda6160b1"
        ],
        "ed9ee88e-75ed-44ea-9549-dc52a5ef5a17": [
            "48e53904-343b-4173-9dda-beadda6160b1"
        ],
        "08ff7efd-9c0f-49aa-87e1-9e581d3b748a": [
            "48e53904-343b-4173-9dda-beadda6160b1"
        ],
        "2bde1bdd-b57d-4380-8abe-cb8d88803626": [
            "48e53904-343b-4173-9dda-beadda6160b1"
        ],
        "bf2f8faf-19cc-4ba4-aab4-c1dd5171e197": [
            "48e53904-343b-4173-9dda-beadda6160b1"
        ],
        "34098566-e766-4bf8-b8e7-fa765fb558d8": [
            "46c05dea-cb33-4fa7-a5ca-aba203f253cc"
        ],
        "7292ff5c-aa59-47a1-9e1d-85ff65356563": [
            "46c05dea-cb33-4fa7-a5ca-aba203f253cc"
        ],
        "3a7df12c-435d-4ad2-b21a-9e6952b1194b": [
            "46c05dea-cb33-4fa7-a5ca-aba203f253cc"
        ],
        "e5278668-376e-442e-814c-9a0e47e55fa0": [
            "46c05dea-cb33-4fa7-a5ca-aba203f253cc"
        ],
        "dd732a98-ed63-4f5e-a98e-5ad86699ddfa": [
            "46c05dea-cb33-4fa7-a5ca-aba203f253cc"
        ],
        "6c731b77-51d8-4679-b4a0-33c7988dfdfc": [
            "c871fb00-732a-4d6c-91a9-1d0fe4c4aedb"
        ],
        "67048347-be1b-4733-bde4-2074b0bbb123": [
            "c871fb00-732a-4d6c-91a9-1d0fe4c4aedb"
        ],
        "42bbd6fa-e26a-4f15-9dd3-e553d18a6dc8": [
            "c871fb00-732a-4d6c-91a9-1d0fe4c4aedb"
        ],
        "5fe721af-06bb-49d2-8212-35dad4e730d0": [
            "c871fb00-732a-4d6c-91a9-1d0fe4c4aedb"
        ],
        "4c6d17b4-c498-410e-b01d-d61d8fb50416": [
            "c871fb00-732a-4d6c-91a9-1d0fe4c4aedb"
        ],
        "72272d2d-8f90-4e3b-b461-703cf24cdfcc": [
            "9b3d2363-20ff-4193-b532-3b92e29e5223"
        ],
        "cc3bbe5c-f167-4e5b-af59-4419bee0eab2": [
            "9b3d2363-20ff-4193-b532-3b92e29e5223"
        ],
        "bed4b6a3-e0a9-4577-a26b-a86386da057a": [
            "9b3d2363-20ff-4193-b532-3b92e29e5223"
        ],
        "d99be5c7-af29-407d-8166-64365cabeb03": [
            "9b3d2363-20ff-4193-b532-3b92e29e5223"
        ],
        "45d294cf-2a04-474d-95a1-794ef4ee70a8": [
            "efb4ae24-c81c-4d82-ae9e-e08d6288d484"
        ],
        "ce5f0912-5f65-4837-aa4c-c6d62d293430": [
            "efb4ae24-c81c-4d82-ae9e-e08d6288d484"
        ],
        "2bb7e1f6-fba0-43c3-9930-f4e3bdda0d3d": [
            "efb4ae24-c81c-4d82-ae9e-e08d6288d484"
        ],
        "3ee56c02-ec7f-41b7-a040-5a478babc475": [
            "efb4ae24-c81c-4d82-ae9e-e08d6288d484"
        ],
        "4700b2b3-04bf-47e5-9ebd-2cbc9deda6db": [
            "efb4ae24-c81c-4d82-ae9e-e08d6288d484"
        ],
        "e9b8bdef-8b3e-488e-943e-7a4be0d05f8f": [
            "35bee546-5ec5-4e6c-9d43-ec2ce1cf3c8c"
        ],
        "2b8bfbe5-27a7-4fc4-91eb-2d80e6e56657": [
            "35bee546-5ec5-4e6c-9d43-ec2ce1cf3c8c"
        ],
        "7ffc00a2-cac5-47b0-83d7-d53298b9ad7d": [
            "35bee546-5ec5-4e6c-9d43-ec2ce1cf3c8c"
        ],
        "62e07244-f653-4eb3-94fd-49d9f97c9483": [
            "35bee546-5ec5-4e6c-9d43-ec2ce1cf3c8c"
        ],
        "94a777d0-e320-49c8-805a-b388051770fc": [
            "35bee546-5ec5-4e6c-9d43-ec2ce1cf3c8c"
        ],
        "d9bf0574-233f-462a-84e3-6b1aa14e3d0b": [
            "3d751562-2ef4-4b50-a37f-f1b223f99a6e"
        ],
        "bafaac0c-72d3-406d-925e-595b24ca0595": [
            "3d751562-2ef4-4b50-a37f-f1b223f99a6e"
        ],
        "dc63e7df-e4cb-4741-9ba6-aa3d5ae400db": [
            "3d751562-2ef4-4b50-a37f-f1b223f99a6e"
        ],
        "8cb0a458-57b5-4725-b01e-46cd4ff92b60": [
            "3d751562-2ef4-4b50-a37f-f1b223f99a6e"
        ],
        "3aec0c62-cc31-42de-a35e-b58277c218a7": [
            "3d751562-2ef4-4b50-a37f-f1b223f99a6e"
        ],
        "895dc213-4fa1-4bd4-b2fd-f84ce48c594f": [
            "3b3dff44-8861-4a4e-a8a7-7d90e02a41ba"
        ],
        "53f97bc5-07a1-4915-9c6d-3a3351032884": [
            "3b3dff44-8861-4a4e-a8a7-7d90e02a41ba"
        ],
        "cad3c50b-37d9-4cc0-b135-cdd4910941ea": [
            "3b3dff44-8861-4a4e-a8a7-7d90e02a41ba"
        ],
        "2ded7486-62f2-414b-a19c-9b71b49aafef": [
            "3b3dff44-8861-4a4e-a8a7-7d90e02a41ba"
        ],
        "40bab1e1-05b2-44fe-a3c5-b3d365d02fcb": [
            "3b3dff44-8861-4a4e-a8a7-7d90e02a41ba"
        ],
        "16ca4211-fab3-4737-878e-85104427de58": [
            "c5b27e27-3d85-429c-859f-44e0197c7185"
        ],
        "b9888aac-a250-42ff-b32e-526a6930596d": [
            "c5b27e27-3d85-429c-859f-44e0197c7185"
        ],
        "d73561c9-549a-4f19-9bed-9946844c6f59": [
            "c5b27e27-3d85-429c-859f-44e0197c7185"
        ],
        "7c30cc68-ce7b-44ba-b7cd-f57f8a397ac8": [
            "c5b27e27-3d85-429c-859f-44e0197c7185"
        ],
        "77713ba4-2bcf-444e-a556-2111fdeca698": [
            "c5b27e27-3d85-429c-859f-44e0197c7185"
        ],
        "4eb32494-c2ec-4f1a-9c66-08d240b228d4": [
            "c5b27e27-3d85-429c-859f-44e0197c7185"
        ],
        "bbe2d7cc-4d98-4e63-bfab-3a689542c19b": [
            "c5b27e27-3d85-429c-859f-44e0197c7185"
        ],
        "0a4bafc4-8dd9-48e1-915c-38b594f34ddf": [
            "c5b27e27-3d85-429c-859f-44e0197c7185"
        ],
        "4d7ba5a4-a074-444f-b011-53104e67b871": [
            "c5b27e27-3d85-429c-859f-44e0197c7185"
        ],
        "f53954ee-63e2-4079-b686-8ef2f3999426": [
            "c5b27e27-3d85-429c-859f-44e0197c7185"
        ],
        "1c4e59b0-edba-4af5-ac18-109b5ede5eb3": [
            "31c1b209-ed10-431a-ba0f-10cdc8c5a620"
        ],
        "75048a0c-ed49-4d09-b5de-7b3ecae60c4f": [
            "31c1b209-ed10-431a-ba0f-10cdc8c5a620"
        ],
        "203822ba-29ac-4d44-93b0-82534bf47aeb": [
            "31c1b209-ed10-431a-ba0f-10cdc8c5a620"
        ],
        "e91532da-a32e-491d-84c2-64c4319cb5b8": [
            "31c1b209-ed10-431a-ba0f-10cdc8c5a620"
        ],
        "b7ee426a-f591-4b26-8136-c79d04ef0e80": [
            "31c1b209-ed10-431a-ba0f-10cdc8c5a620"
        ],
        "c0c2bf5e-6922-4dc8-bd93-fa92ce75affe": [
            "4feaef2b-cfc3-4602-854c-510e9f3d4b55"
        ],
        "4a55e876-ba63-4087-b39d-7f9d4212e533": [
            "17a1069e-f779-44b9-aecf-b80dc7270f15"
        ],
        "0599110d-91d6-4134-a3eb-54bffffa7142": [
            "17a1069e-f779-44b9-aecf-b80dc7270f15"
        ],
        "20b06a08-b257-4448-9d86-49f16952ee1c": [
            "17a1069e-f779-44b9-aecf-b80dc7270f15"
        ],
        "05e03b51-b193-4ba1-9040-d0707622969b": [
            "17a1069e-f779-44b9-aecf-b80dc7270f15"
        ],
        "b4f34d6e-50c8-4fd9-a90f-9684d7ae135b": [
            "17a1069e-f779-44b9-aecf-b80dc7270f15"
        ],
        "3dd7283a-b511-4dac-bfb3-3288bdedfcd7": [
            "27aaef43-9da6-4e44-90bc-70dee239a8e8"
        ],
        "151f5ad9-3b30-49a8-90c0-53a743f0be6b": [
            "27aaef43-9da6-4e44-90bc-70dee239a8e8"
        ],
        "684aa1b5-3243-47d0-a553-6c89aa925a04": [
            "27aaef43-9da6-4e44-90bc-70dee239a8e8"
        ],
        "9348aa78-8dba-4399-8cda-209e89440b89": [
            "27aaef43-9da6-4e44-90bc-70dee239a8e8"
        ],
        "85add476-0105-48f7-8737-c593a3ee7161": [
            "27aaef43-9da6-4e44-90bc-70dee239a8e8"
        ],
        "fbd84158-455d-49ca-9cf3-a77ca3db20bc": [
            "27aaef43-9da6-4e44-90bc-70dee239a8e8"
        ],
        "e0ea868d-b22b-4294-bd58-6c4d9e774707": [
            "27aaef43-9da6-4e44-90bc-70dee239a8e8"
        ],
        "e73f7c57-67d0-4e03-a9ca-e5257c47bc25": [
            "27aaef43-9da6-4e44-90bc-70dee239a8e8"
        ],
        "8e2318b1-9bc0-47aa-a5f5-aecd15dc971c": [
            "27aaef43-9da6-4e44-90bc-70dee239a8e8"
        ],
        "46fb61b2-fcb9-4c57-91f1-7b852d6fef75": [
            "27aaef43-9da6-4e44-90bc-70dee239a8e8"
        ],
        "f671b654-f92c-4956-ae5e-e0d700e4c2f6": [
            "3073d447-a4e3-489f-8913-107e29cb186d"
        ],
        "a7204bb2-9d2f-483f-b69f-37b9930b5ef7": [
            "3073d447-a4e3-489f-8913-107e29cb186d"
        ],
        "c3eec421-8eeb-4d7c-9001-0897dee9a747": [
            "3073d447-a4e3-489f-8913-107e29cb186d"
        ],
        "403256a2-f4b1-4788-a494-ef320606505e": [
            "3073d447-a4e3-489f-8913-107e29cb186d"
        ],
        "8fd7df84-0349-419a-9eaf-0ee9e14d7c20": [
            "3073d447-a4e3-489f-8913-107e29cb186d"
        ],
        "f6e5df59-f553-4f8d-a1da-54175398c406": [
            "954f2c99-3805-4ccc-974d-50e6dd2451b3"
        ],
        "8a8adc7d-c22c-4bc4-9576-db0f1b793718": [
            "954f2c99-3805-4ccc-974d-50e6dd2451b3"
        ],
        "95a0e4b3-9a2d-4003-b02b-8764daf0c38f": [
            "954f2c99-3805-4ccc-974d-50e6dd2451b3"
        ],
        "082ff025-2049-4134-aafc-265c17c46123": [
            "954f2c99-3805-4ccc-974d-50e6dd2451b3"
        ],
        "13344cab-1451-4c6e-9e71-643119b64d0c": [
            "954f2c99-3805-4ccc-974d-50e6dd2451b3"
        ],
        "3ec6abcb-89d6-48b4-ad8c-09a7a42e9c2c": [
            "02042ab9-7641-4b93-8eb4-d5e7ea74014a"
        ],
        "6f46469e-d6e5-4cb3-a712-06e39c68547b": [
            "02042ab9-7641-4b93-8eb4-d5e7ea74014a"
        ],
        "2b2e513b-e8ee-487a-9d6a-ea4e8520f06c": [
            "02042ab9-7641-4b93-8eb4-d5e7ea74014a"
        ],
        "f678dfc8-fb98-4245-b110-8ca384b462d3": [
            "02042ab9-7641-4b93-8eb4-d5e7ea74014a"
        ],
        "3e980cee-b124-44ea-aea0-89739681d5b9": [
            "02042ab9-7641-4b93-8eb4-d5e7ea74014a"
        ],
        "96e19324-83f1-4dcd-b884-52eb516ed016": [
            "02042ab9-7641-4b93-8eb4-d5e7ea74014a"
        ],
        "35a91271-1ab5-48dd-b12c-08a30c44b5c0": [
            "02042ab9-7641-4b93-8eb4-d5e7ea74014a"
        ],
        "a5b3f360-defe-48f7-b07a-abba001d54f6": [
            "02042ab9-7641-4b93-8eb4-d5e7ea74014a"
        ],
        "e545fca2-101c-4625-af73-7b79b649ea68": [
            "02042ab9-7641-4b93-8eb4-d5e7ea74014a"
        ],
        "3450ff10-7b37-4d61-ba28-acdd3be1f9b7": [
            "02042ab9-7641-4b93-8eb4-d5e7ea74014a"
        ],
        "251a2097-b0e8-447a-934a-2b66c426492f": [
            "b7794895-6a00-413f-af52-0bd007dfdd20"
        ],
        "7cf11b42-0690-4cd4-8802-68b53a46c60d": [
            "6e4c92b7-bf7c-40d2-b716-b38e5d8065e2"
        ],
        "7eaa4245-3f97-4be8-80e4-0e1021f98982": [
            "4d033d08-b2aa-4591-b3aa-7ef013da5e47"
        ],
        "5c4d7010-ea72-495a-88de-0acc429d35dc": [
            "ea33af40-1308-4eb9-8ea3-5983ffd84ccf"
        ],
        "41e0a88e-15c0-47ef-b6be-4a1e5760644c": [
            "9eb6b1c7-40c2-463b-ae00-61432d36be0a"
        ],
        "e8bf3401-4fed-4e48-a3b2-6b66f75beb6f": [
            "87ab3b1d-e3e8-4a94-8866-c77e60362507"
        ],
        "acfc22b1-9681-4e8c-a8bb-1f33f63aa0b5": [
            "3b2895af-baa1-4679-ba42-7f2411a5a1a1"
        ],
        "d7154a11-8e6d-42b8-85ea-11d72b730003": [
            "3b2895af-baa1-4679-ba42-7f2411a5a1a1"
        ],
        "e819bd68-a930-4aff-9c9f-cbe71d800ef2": [
            "3b2895af-baa1-4679-ba42-7f2411a5a1a1"
        ],
        "31154a49-1309-47ba-b165-6322bb9e0ab3": [
            "3b2895af-baa1-4679-ba42-7f2411a5a1a1"
        ],
        "c680bd09-fb26-4064-bdc4-9b4b64112d16": [
            "3b2895af-baa1-4679-ba42-7f2411a5a1a1"
        ],
        "e22cb956-dc41-4618-92c5-a4dbcf7dac5a": [
            "7de78b96-4a74-481e-ba0e-d7fdf7f5bced"
        ],
        "ca4acfd6-3966-4390-960c-fefd1cbe4205": [
            "bf9b0fa8-9388-4f5e-9675-c3a8259d7f07"
        ],
        "18d92181-732c-4a05-9c64-614f8cb1f22b": [
            "bf9b0fa8-9388-4f5e-9675-c3a8259d7f07"
        ],
        "923a4c74-adb0-4a67-897a-5fe51865888b": [
            "bf9b0fa8-9388-4f5e-9675-c3a8259d7f07"
        ],
        "8e525723-0fe8-4566-8824-9536161736b4": [
            "bf9b0fa8-9388-4f5e-9675-c3a8259d7f07"
        ],
        "fc93ee87-fce0-4189-8d49-d82e52720b20": [
            "bf9b0fa8-9388-4f5e-9675-c3a8259d7f07"
        ]
    },
    "mode": "text"
}